{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install required packages"
      ],
      "metadata": {
        "id": "Vpw-pkewCVks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install calflops\n",
        "!apt-get -qq install xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "!pip install onnx onnxruntime\n",
        "!git clone https://github.com/onnx/onnx-tensorflow.git && cd onnx-tensorflow && pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if1-sLljCBi0",
        "outputId": "a28f513c-795b-4fb9-ef44-3c4f866fc04b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: calflops in /usr/local/lib/python3.10/dist-packages (0.2.9)\n",
            "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from calflops) (0.25.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from calflops) (0.20.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from calflops) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->calflops) (0.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.16.4->calflops) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1.0->calflops) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1.0->calflops) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.16.4->calflops) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1.0->calflops) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.15.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mfatal: destination path 'onnx-tensorflow' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./onnx-tensorflow\")"
      ],
      "metadata": {
        "id": "e6bslSW5CMUH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install onnx-simplifier"
      ],
      "metadata": {
        "id": "jlGKIskrCR43"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/PINTO0309/onnx2tf/releases/download/1.16.31/flatc.tar.gz \\\n",
        "  && tar -zxvf flatc.tar.gz \\\n",
        "  && sudo chmod +x flatc \\\n",
        "  && sudo mv flatc /usr/bin/\n",
        "!pip install -U pip \\\n",
        "  && pip install -U onnx>=1.14.1 \\\n",
        "  && python -m pip install onnx_graphsurgeon \\\n",
        "        --index-url https://pypi.ngc.nvidia.com \\\n",
        "  && pip install -U onnxruntime==1.16.0 \\\n",
        "  && pip install -U onnxsim==0.4.33 \\\n",
        "  && pip install -U simple_onnx_processing_tools \\\n",
        "  && pip install -U onnx2tf \\\n",
        "  && pip install -U protobuf==3.20.3 \\\n",
        "  && pip install -U h5py==3.7.0 \\\n",
        "  && pip install -U psutil==5.9.5 \\\n",
        "  && pip install -U ml_dtypes==0.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrG2WY_YCU35",
        "outputId": "6efb929b-52eb-40d2-f9a7-be2daead0eda"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-03 10:00:09--  https://github.com/PINTO0309/onnx2tf/releases/download/1.16.31/flatc.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/541831874/29499355-44ab-4fb6-86c8-582f4bad68a3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240103T100009Z&X-Amz-Expires=300&X-Amz-Signature=6b5b8c5c3311d9082ad7f692b2ad3b98e6b398840d928275c9148887f9ab783d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=541831874&response-content-disposition=attachment%3B%20filename%3Dflatc.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-01-03 10:00:09--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/541831874/29499355-44ab-4fb6-86c8-582f4bad68a3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240103%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240103T100009Z&X-Amz-Expires=300&X-Amz-Signature=6b5b8c5c3311d9082ad7f692b2ad3b98e6b398840d928275c9148887f9ab783d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=541831874&response-content-disposition=attachment%3B%20filename%3Dflatc.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1382707 (1.3M) [application/octet-stream]\n",
            "Saving to: ‘flatc.tar.gz.1’\n",
            "\n",
            "flatc.tar.gz.1      100%[===================>]   1.32M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-01-03 10:00:10 (22.8 MB/s) - ‘flatc.tar.gz.1’ saved [1382707/1382707]\n",
            "\n",
            "flatc\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: onnx_graphsurgeon in /usr/local/lib/python3.10/dist-packages (0.3.27)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon) (1.23.5)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from onnx_graphsurgeon) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->onnx_graphsurgeon) (3.20.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: onnxruntime==1.16.0 in /usr/local/lib/python3.10/dist-packages (1.16.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.16.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.16.0) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.16.0) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.16.0) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.16.0) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.16.0) (1.12)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime==1.16.0) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.16.0) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: onnxsim==0.4.33 in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (from onnxsim==0.4.33) (1.15.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnxsim==0.4.33) (13.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx->onnxsim==0.4.33) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->onnxsim==0.4.33) (3.20.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim==0.4.33) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim==0.4.33) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim==0.4.33) (0.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: simple_onnx_processing_tools in /usr/local/lib/python3.10/dist-packages (1.1.30)\n",
            "Requirement already satisfied: snc4onnx>=1.0.12 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.12)\n",
            "Requirement already satisfied: sne4onnx>=1.0.11 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.11)\n",
            "Requirement already satisfied: snd4onnx>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.1.6)\n",
            "Requirement already satisfied: scs4onnx>=1.0.18 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.18)\n",
            "Requirement already satisfied: sog4onnx>=1.0.16 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.16)\n",
            "Requirement already satisfied: sam4onnx>=1.0.14 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.14)\n",
            "Requirement already satisfied: soc4onnx>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.2)\n",
            "Requirement already satisfied: scc4onnx>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.5)\n",
            "Requirement already satisfied: sna4onnx>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.6)\n",
            "Requirement already satisfied: sbi4onnx>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.5)\n",
            "Requirement already satisfied: sor4onnx>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.5)\n",
            "Requirement already satisfied: sit4onnx>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.7)\n",
            "Requirement already satisfied: onnx2json>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (2.0.4)\n",
            "Requirement already satisfied: json2onnx>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (2.0.3)\n",
            "Requirement already satisfied: sed4onnx>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.5)\n",
            "Requirement already satisfied: soa4onnx>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.4)\n",
            "Requirement already satisfied: sod4onnx>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.0)\n",
            "Requirement already satisfied: ssi4onnx>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.2)\n",
            "Requirement already satisfied: ssc4onnx>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.8)\n",
            "Requirement already satisfied: sio4onnx>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.2)\n",
            "Requirement already satisfied: svs4onnx>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.0)\n",
            "Requirement already satisfied: onnx2tf>=1.17.6 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.19.4)\n",
            "Requirement already satisfied: sng4onnx>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.1)\n",
            "Requirement already satisfied: sde4onnx>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.0)\n",
            "Requirement already satisfied: spo4onnx>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from simple_onnx_processing_tools) (1.0.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: onnx2tf in /usr/local/lib/python3.10/dist-packages (1.19.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: protobuf==3.20.3 in /usr/local/lib/python3.10/dist-packages (3.20.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: h5py==3.7.0 in /usr/local/lib/python3.10/dist-packages (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.10/dist-packages (from h5py==3.7.0) (1.23.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: psutil==5.9.5 in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: ml_dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ml_dtypes==0.2.0) (1.23.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    # torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "E4hRjLejmepm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data"
      ],
      "metadata": {
        "id": "l5ToCLX_Cfie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define the directory containing the CSV files\n",
        "directory = \"./data\" # Replace with the actual directory path\n",
        "\n",
        "def prepare_data():\n",
        "  # Initialize an empty list to store the data\n",
        "  data = []\n",
        "\n",
        "  # List of CSV files\n",
        "  csv_files = [\"AlohomoraCharm.csv\", \"ArrestoMomentumCharm.csv\", \"AvadaKedavra.csv\", \"LocomotorCharm.csv\", \"Revelio.csv\"]\n",
        "\n",
        "  # Loop through each file\n",
        "  for file in csv_files:\n",
        "      # Construct the full path to the file\n",
        "      file_path = os.path.join(directory, file)\n",
        "\n",
        "      # Read the CSV file, skipping empty rows (which are interpreted as NaN)\n",
        "      df = pd.read_csv(file_path, header=0)\n",
        "\n",
        "      # Drop NaN rows\n",
        "      df = df.dropna()\n",
        "      df['aX'] = (df['aX']+4)/8\n",
        "      df['aY'] = (df['aY']+4)/8\n",
        "      df['aZ'] = (df['aZ']+4)/8\n",
        "\n",
        "      df['gX'] = (df['gX']+2000)/4000\n",
        "      df['gY'] = (df['gY']+2000)/4000\n",
        "      df['gZ'] = (df['gZ']+2000)/4000\n",
        "\n",
        "      # Split the dataframe into samples of 119 rows each\n",
        "      for i in range(0, len(df), 119):\n",
        "          sample = df.iloc[i:i+119]\n",
        "\n",
        "          # Convert the sample dataframe to a numpy array\n",
        "          sample_array = sample.to_numpy()\n",
        "\n",
        "          # Get the label from the file name\n",
        "          label = file.replace(\".csv\", \"\")\n",
        "\n",
        "          # Append the label and data to the list\n",
        "          data.append({'label': label, 'data': sample_array})\n",
        "\n",
        "  # Create a final dataframe\n",
        "  final_df = pd.DataFrame(data)\n",
        "  return final_df"
      ],
      "metadata": {
        "id": "SkxOkhMv65me"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = prepare_data()"
      ],
      "metadata": {
        "id": "oaAWlyp0-5QO"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {len(df.index)} samples in total.')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labels = df[\"label\"].unique()\n",
        "encoder = LabelEncoder()\n",
        "df[\"ClassID\"] = encoder.fit_transform(df[\"label\"])\n",
        "\n",
        "df[\"data\"] = df[\"data\"].apply(lambda x: x.transpose((1, 0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1vk8RP973sL",
        "outputId": "a21fad3a-0189-41f1-e436-01e0ecaead2d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 125 samples in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.inverse_transform([0,1,2,3,4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mexmik73-cZJ",
        "outputId": "5f1319a2-7ab0-4892-b989-5b82bf8125eb"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AlohomoraCharm', 'ArrestoMomentumCharm', 'AvadaKedavra',\n",
              "       'LocomotorCharm', 'Revelio'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0][\"data\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9qjMsLx-kZC",
        "outputId": "ef58af8a-b5d2-452a-c640-f49b88ec0ff7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 119)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Z98SEY0H-n1F"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of training set: {len(df_train.index)}\")\n",
        "print(f\"Number of test set: {len(df_test.index)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-4ak190-tdw",
        "outputId": "033fe040-ffcb-40a2-91b7-a39fc6776190"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training set: 100\n",
            "Number of test set: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset"
      ],
      "metadata": {
        "id": "uxRXDQ8e_XgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "dCSerHfn_WoI"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class GestureDataset(Dataset):\n",
        "    def __init__(self, df, transform, preload=False, device='cpu'):\n",
        "        self.df = df\n",
        "        self.num_classes = self.df[\"ClassID\"].nunique()\n",
        "        self.transform = transform\n",
        "        self.preload = preload\n",
        "        self.data = []\n",
        "\n",
        "        if self.preload:\n",
        "            for index in tqdm(range(len(self.df.index)), f\"Preloading dataset to {device}\"):\n",
        "                data = self.df.iloc[index]['data']\n",
        "                data = torch.tensor(data, dtype=torch.float32)\n",
        "                label = self.df.iloc[index][\"ClassID\"]\n",
        "\n",
        "                data = data.to(device)  # Move to the specified device\n",
        "                self.data.append((data, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df.index)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.preload:\n",
        "            return self.data[index]\n",
        "\n",
        "        data = self.df.iloc[index]['data']\n",
        "        data = torch.tensor(data, dtype=torch.float32)\n",
        "\n",
        "        label = self.df.iloc[index][\"ClassID\"]\n",
        "\n",
        "        return data, label"
      ],
      "metadata": {
        "id": "DFIRwGpR_RMN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zognLFdW_w0t",
        "outputId": "91b7d997-1a35-443d-86ce-5cb4de3b00b1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Transforms\n",
        "# transform = transforms.Compose([\n",
        "#     # transforms.Resize((32, 32)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# # Create dataset and dataloaders\n",
        "# train_dataset = GestureDataset(df=df_train, transform=transform, preload=True, device=device)\n",
        "\n",
        "# test_dataset = GestureDataset(df=df_test, transform=transform, preload=True, device=device)"
      ],
      "metadata": {
        "id": "bC4moDrl_0PD"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add data augmentations"
      ],
      "metadata": {
        "id": "j-zN0DLTqIW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "class AddRandomNoise(object):\n",
        "    def __init__(self, noise_level=0.05):\n",
        "        self.noise_level = noise_level\n",
        "\n",
        "    def __call__(self, data):\n",
        "        noise = torch.randn_like(data) * self.noise_level\n",
        "        return data + noise\n",
        "\n",
        "class TimeWarp(object):\n",
        "    def __init__(self, warp_factor=0.1):\n",
        "        self.warp_factor = warp_factor\n",
        "\n",
        "    def __call__(self, data):\n",
        "        time_steps = data.shape[0]\n",
        "        warp = np.random.uniform(1 - self.warp_factor, 1 + self.warp_factor)\n",
        "        new_time_steps = int(time_steps * warp)\n",
        "        data = torch.nn.functional.interpolate(data.view(1, 1, -1), size=new_time_steps, mode='linear', align_corners=False)\n",
        "        return data.view(-1)\n",
        "\n",
        "class ScaleTransform(object):\n",
        "    def __init__(self, scale_factor=1.2):\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def __call__(self, data):\n",
        "        return data * self.scale_factor\n",
        "\n",
        "# Define your transform pipeline\n",
        "transform_train = transforms.Compose([\n",
        "    AddRandomNoise(noise_level=0.05),\n",
        "    TimeWarp(warp_factor=0.1),\n",
        "    ScaleTransform(scale_factor=1.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "train_dataset = GestureDataset(df=df_train, transform=transform_train, preload=True, device=device)\n",
        "test_dataset = GestureDataset(df=df_test, transform=transform_test, preload=True, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7703kIdB5tL_",
        "outputId": "2f600ae5-73b3-4213-e9d1-842327d4faa8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preloading dataset to cpu: 100%|██████████| 100/100 [00:00<00:00, 3511.61it/s]\n",
            "Preloading dataset to cpu: 100%|██████████| 25/25 [00:00<00:00, 3085.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set_seed(666)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "print(f'Num of classes: {train_dataset.num_classes}')\n",
        "num_classes = train_dataset.num_classes\n",
        "# Example: Print the shape of first batch data and labels\n",
        "for data, labels in train_loader:\n",
        "    print(data.shape,  labels.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sLrweV9ASvS",
        "outputId": "a263169c-340f-49aa-9b38-8542bbcc5295"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of classes: 5\n",
            "torch.Size([100, 6, 119]) torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot examples"
      ],
      "metadata": {
        "id": "DOGYOKq9AgDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = [\n",
        "    'aX','aY','aZ',\n",
        "    'gX','gY','gZ',\n",
        "]"
      ],
      "metadata": {
        "id": "3JUjLnyL40rr"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "sample = next(iter(train_loader))\n",
        "sample_data = sample[0].permute(0, 2, 1)\n",
        "sample_label = sample[1]\n",
        "\n",
        "num_samples_to_plot = 4\n",
        "indices = np.random.choice(sample_data.shape[0], num_samples_to_plot, replace=False)\n",
        "\n",
        "plt.figure(figsize=(18, 15))\n",
        "\n",
        "for i, index in enumerate(indices, 1):\n",
        "    plt.subplot(num_samples_to_plot, 1, i)\n",
        "    for j in range(sample_data.shape[2]):\n",
        "        plt.plot(sample_data[index, :, j].numpy(), label=f'{feature_names[j]}')\n",
        "    plt.legend()\n",
        "    plt.title(f'Sample {index} Class:{encoder.inverse_transform([sample_label[index]])}')\n",
        "    # plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e3cZiwq2AfUt",
        "outputId": "a376a973-55ff-41bd-ae7e-2e5fa372669b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x1500 with 4 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"1048.065625pt\" height=\"877.79625pt\" viewBox=\"0 0 1048.065625 877.79625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-01-03T10:37:44.545502</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 877.79625 \nL 1048.065625 877.79625 \nL 1048.065625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 203.100734 \nL 1040.865625 203.100734 \nL 1040.865625 22.318125 \nL 36.465625 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m0a4955e443\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"82.12017\" y=\"203.100734\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(78.93892 217.699171) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"236.881341\" y=\"203.100734\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(230.518841 217.699171) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"391.642513\" y=\"203.100734\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(385.280013 217.699171) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"546.403684\" y=\"203.100734\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(540.041184 217.699171) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"701.164855\" y=\"203.100734\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(694.802355 217.699171) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"855.926026\" y=\"203.100734\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(846.382276 217.699171) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"1010.687197\" y=\"203.100734\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120 -->\n      <g transform=\"translate(1001.143447 217.699171) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"mc47b2e7a22\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"192.656292\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.4 -->\n      <g transform=\"translate(13.5625 196.45551) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"159.663003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.5 -->\n      <g transform=\"translate(13.5625 163.462221) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"126.669714\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.6 -->\n      <g transform=\"translate(13.5625 130.468932) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"93.676425\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.7 -->\n      <g transform=\"translate(13.5625 97.475644) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"60.683136\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(13.5625 64.482355) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"27.689847\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.9 -->\n      <g transform=\"translate(13.5625 31.489066) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path d=\"M 82.12017 122.586787 \nL 89.858229 121.679479 \nL 97.596288 120.813409 \nL 105.334346 119.576152 \nL 113.072405 118.916294 \nL 120.810463 118.421391 \nL 128.548522 118.132701 \nL 136.28658 118.46263 \nL 144.024639 119.122507 \nL 151.762697 120.359744 \nL 159.500756 122.050646 \nL 167.238815 123.865283 \nL 174.976873 125.803636 \nL 182.714932 127.659511 \nL 190.45299 129.43291 \nL 198.191049 131.16505 \nL 205.929107 133.103403 \nL 213.667166 134.91804 \nL 221.405224 136.897631 \nL 229.143283 138.464817 \nL 236.881341 139.867029 \nL 244.6194 141.516691 \nL 252.357459 143.042638 \nL 260.095517 144.44485 \nL 267.833576 145.723345 \nL 275.571634 147.04306 \nL 283.309693 148.156601 \nL 291.047751 149.393838 \nL 298.78581 150.383644 \nL 306.523868 151.208476 \nL 314.261927 151.827094 \nL 321.999986 152.363236 \nL 329.738044 152.734403 \nL 337.476103 152.858138 \nL 345.214161 153.064351 \nL 352.95222 153.311803 \nL 360.690278 153.641731 \nL 368.428337 154.054137 \nL 376.166395 154.466563 \nL 383.904454 154.920227 \nL 391.642513 155.497607 \nL 399.380571 155.99249 \nL 407.11863 156.074987 \nL 414.856688 156.322438 \nL 422.594747 156.322438 \nL 430.332805 156.239941 \nL 438.070864 155.827535 \nL 445.808922 155.208917 \nL 453.546981 154.714014 \nL 461.285039 154.219111 \nL 469.023098 153.353041 \nL 476.761157 152.486971 \nL 484.499215 151.455927 \nL 492.237274 150.713573 \nL 499.975332 149.393838 \nL 507.713391 147.62046 \nL 515.451449 146.795628 \nL 523.189508 145.929539 \nL 530.927566 145.393397 \nL 538.665625 145.187204 \nL 546.403684 144.403611 \nL 554.141742 143.29009 \nL 561.879801 141.84662 \nL 569.617859 139.454623 \nL 577.355918 136.155277 \nL 585.093976 132.979687 \nL 592.832035 129.185458 \nL 600.570093 125.308733 \nL 608.308152 121.514504 \nL 616.046211 118.46263 \nL 623.784269 115.86442 \nL 631.522328 112.68881 \nL 639.260386 109.348246 \nL 646.998445 105.76021 \nL 654.736503 102.955787 \nL 662.474562 100.976196 \nL 670.21262 99.780177 \nL 677.950679 100.027629 \nL 685.688737 101.42984 \nL 693.426796 104.110547 \nL 701.164855 106.667539 \nL 708.902913 109.760652 \nL 716.640972 112.895023 \nL 724.37903 115.823162 \nL 732.117089 118.17394 \nL 739.855147 120.524719 \nL 747.593206 123.65907 \nL 755.331264 126.174823 \nL 763.069323 129.43291 \nL 770.807382 132.567261 \nL 778.54544 135.577897 \nL 786.283499 138.38234 \nL 794.021557 141.021789 \nL 801.759616 143.908708 \nL 809.497674 146.506938 \nL 817.235733 149.105148 \nL 824.973791 151.538404 \nL 832.71185 153.517996 \nL 840.449909 155.332633 \nL 848.187967 157.14727 \nL 855.926026 158.508243 \nL 863.664084 159.45679 \nL 871.402143 159.951693 \nL 879.140201 160.652798 \nL 886.87826 160.982737 \nL 894.616318 161.395153 \nL 902.354377 161.642604 \nL 910.092435 161.848807 \nL 917.830494 161.972533 \nL 925.568553 162.137497 \nL 933.306611 161.972533 \nL 941.04467 161.395153 \nL 948.782728 160.570321 \nL 956.520787 159.827967 \nL 964.258845 159.415551 \nL 971.996904 158.961887 \nL 979.734962 158.425746 \nL 987.473021 158.137056 \nL 995.21108 157.518437 \n\" clip-path=\"url(#p4053fe7a9f)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 82.12017 133.309616 \nL 89.858229 127.865724 \nL 97.596288 125.185017 \nL 105.334346 124.81385 \nL 113.072405 124.690114 \nL 120.810463 125.020043 \nL 128.548522 126.422254 \nL 136.28658 128.154414 \nL 144.024639 128.979246 \nL 151.762697 128.690556 \nL 159.500756 128.443104 \nL 167.238815 126.463513 \nL 174.976873 122.586787 \nL 182.714932 118.091463 \nL 190.45299 114.462189 \nL 198.191049 113.719854 \nL 205.929107 115.616969 \nL 213.667166 119.534913 \nL 221.405224 126.174823 \nL 229.143283 133.969473 \nL 236.881341 139.990744 \nL 244.6194 143.826231 \nL 252.357459 146.135751 \nL 260.095517 147.867911 \nL 267.833576 149.517574 \nL 275.571634 151.290953 \nL 283.309693 153.270544 \nL 291.047751 154.878969 \nL 298.78581 157.229747 \nL 306.523868 160.157905 \nL 314.261927 163.910886 \nL 321.999986 167.622637 \nL 329.738044 170.71575 \nL 337.476103 171.87052 \nL 345.214161 171.251891 \nL 352.95222 170.550785 \nL 360.690278 170.303334 \nL 368.428337 170.014644 \nL 376.166395 169.354777 \nL 383.904454 168.529946 \nL 391.642513 168.653671 \nL 399.380571 170.179618 \nL 407.11863 172.117971 \nL 414.856688 175.705987 \nL 422.594747 180.325047 \nL 430.332805 184.655417 \nL 438.070864 184.944107 \nL 445.808922 181.603542 \nL 453.546981 176.77827 \nL 461.285039 173.02528 \nL 469.023098 170.097131 \nL 476.761157 167.870078 \nL 484.499215 166.055451 \nL 492.237274 164.983168 \nL 499.975332 163.869647 \nL 507.713391 161.023976 \nL 515.451449 159.126861 \nL 523.189508 158.796933 \nL 530.927566 159.044384 \nL 538.665625 158.920648 \nL 546.403684 157.064773 \nL 554.141742 153.806686 \nL 561.879801 151.95083 \nL 569.617859 153.517996 \nL 577.355918 156.528631 \nL 585.093976 159.992931 \nL 592.832035 163.292267 \nL 600.570093 164.07586 \nL 608.308152 162.591161 \nL 616.046211 158.137056 \nL 623.784269 153.847944 \nL 631.522328 150.589857 \nL 639.260386 146.919344 \nL 646.998445 141.063027 \nL 654.736503 134.835543 \nL 662.474562 128.484343 \nL 670.21262 122.586787 \nL 677.950679 118.792559 \nL 685.688737 115.86442 \nL 693.426796 114.214737 \nL 701.164855 112.895023 \nL 708.902913 110.915412 \nL 716.640972 109.389484 \nL 724.37903 108.06975 \nL 732.117089 106.873732 \nL 739.855147 104.89414 \nL 747.593206 103.780619 \nL 755.331264 105.389043 \nL 763.069323 107.822299 \nL 770.807382 110.37927 \nL 778.54544 113.101216 \nL 786.283499 116.070613 \nL 794.021557 118.916294 \nL 801.759616 122.421833 \nL 809.497674 126.917157 \nL 817.235733 132.196094 \nL 824.973791 137.639986 \nL 832.71185 142.877664 \nL 840.449909 147.455486 \nL 848.187967 152.52821 \nL 855.926026 158.137056 \nL 863.664084 163.49847 \nL 871.402143 167.663875 \nL 879.140201 170.179618 \nL 886.87826 172.901564 \nL 894.616318 175.747235 \nL 902.354377 178.22173 \nL 910.092435 179.830154 \nL 917.830494 180.53126 \nL 925.568553 181.191127 \nL 933.306611 180.696224 \nL 941.04467 180.24257 \nL 948.782728 177.025721 \nL 956.520787 174.097563 \nL 964.258845 171.705555 \nL 971.996904 169.56099 \nL 979.734962 165.643035 \nL 987.473021 161.436391 \nL 995.21108 158.920648 \n\" clip-path=\"url(#p4053fe7a9f)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 82.12017 97.96556 \nL 89.858229 89.139846 \nL 97.596288 85.139405 \nL 105.334346 84.190857 \nL 113.072405 85.056927 \nL 120.810463 88.974872 \nL 128.548522 94.666215 \nL 136.28658 98.996585 \nL 144.024639 102.130956 \nL 151.762697 103.698141 \nL 159.500756 104.193044 \nL 167.238815 103.73938 \nL 174.976873 104.770424 \nL 182.714932 108.812104 \nL 190.45299 113.678616 \nL 198.191049 118.75132 \nL 205.929107 123.205426 \nL 213.667166 127.082131 \nL 221.405224 131.659953 \nL 229.143283 136.402729 \nL 236.881341 139.083436 \nL 244.6194 140.361931 \nL 252.357459 141.063027 \nL 260.095517 142.13531 \nL 267.833576 143.166354 \nL 275.571634 144.568566 \nL 283.309693 145.682087 \nL 291.047751 146.919344 \nL 298.78581 149.187626 \nL 306.523868 152.157023 \nL 314.261927 156.404916 \nL 321.999986 160.157905 \nL 329.738044 163.622196 \nL 337.476103 166.839044 \nL 345.214161 169.932167 \nL 352.95222 172.15921 \nL 360.690278 174.716191 \nL 368.428337 177.314411 \nL 376.166395 179.252764 \nL 383.904454 180.160083 \nL 391.642513 179.582703 \nL 399.380571 178.180481 \nL 407.11863 177.64435 \nL 414.856688 177.850553 \nL 422.594747 178.386694 \nL 430.332805 177.809314 \nL 438.070864 176.118403 \nL 445.808922 172.035484 \nL 453.546981 167.292698 \nL 461.285039 162.426187 \nL 469.023098 159.333074 \nL 476.761157 157.188508 \nL 484.499215 156.404916 \nL 492.237274 157.559676 \nL 499.975332 160.570321 \nL 507.713391 162.426187 \nL 515.451449 160.116657 \nL 523.189508 154.095376 \nL 530.927566 145.805823 \nL 538.665625 136.485226 \nL 546.403684 127.741989 \nL 554.141742 119.741126 \nL 561.879801 112.523836 \nL 569.617859 107.244919 \nL 577.355918 103.038265 \nL 585.093976 100.110126 \nL 592.832035 99.945152 \nL 600.570093 100.769983 \nL 608.308152 100.068867 \nL 616.046211 93.92388 \nL 623.784269 81.592627 \nL 631.522328 67.611732 \nL 639.260386 56.888903 \nL 646.998445 45.877405 \nL 654.736503 38.866328 \nL 662.474562 34.577196 \nL 670.21262 31.607799 \nL 677.950679 30.535516 \nL 685.688737 32.927533 \nL 693.426796 39.773636 \nL 701.164855 50.661419 \nL 708.902913 62.992672 \nL 716.640972 75.777569 \nL 724.37903 90.047155 \nL 732.117089 102.667097 \nL 739.855147 112.771287 \nL 747.593206 122.133143 \nL 755.331264 124.896327 \nL 763.069323 126.999654 \nL 770.807382 127.865724 \nL 778.54544 129.639103 \nL 786.283499 132.154856 \nL 794.021557 134.794304 \nL 801.759616 137.22756 \nL 809.497674 139.5371 \nL 817.235733 142.094071 \nL 824.973791 144.980991 \nL 832.71185 148.032866 \nL 840.449909 150.754811 \nL 848.187967 153.724208 \nL 855.926026 156.446154 \nL 863.664084 159.291835 \nL 871.402143 162.178745 \nL 879.140201 164.859443 \nL 886.87826 167.25146 \nL 894.616318 169.066087 \nL 902.354377 170.756998 \nL 910.092435 171.623068 \nL 917.830494 172.200448 \nL 925.568553 173.520183 \nL 933.306611 176.943234 \nL 941.04467 182.015958 \nL 948.782728 186.635018 \nL 956.520787 190.676698 \nL 964.258845 193.068705 \nL 971.996904 194.883342 \nL 979.734962 193.769811 \nL 987.473021 190.882901 \nL 995.21108 188.697097 \n\" clip-path=\"url(#p4053fe7a9f)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 82.12017 159.068966 \nL 89.858229 159.668037 \nL 97.596288 160.176539 \nL 105.334346 160.584254 \nL 113.072405 160.765561 \nL 120.810463 160.846061 \nL 128.548522 160.851096 \nL 136.28658 160.992058 \nL 144.024639 161.198507 \nL 151.762697 161.414946 \nL 159.500756 161.691847 \nL 167.238815 161.973772 \nL 174.976873 162.240603 \nL 182.714932 162.331252 \nL 190.45299 162.396659 \nL 198.191049 162.567818 \nL 205.929107 162.754139 \nL 213.667166 162.809488 \nL 221.405224 162.693687 \nL 229.143283 162.517504 \nL 236.881341 162.351379 \nL 244.6194 162.220476 \nL 252.357459 162.1097 \nL 260.095517 162.029199 \nL 267.833576 161.998924 \nL 275.571634 161.968737 \nL 283.309693 161.993899 \nL 291.047751 162.008993 \nL 298.78581 161.958678 \nL 306.523868 161.86803 \nL 314.261927 161.767401 \nL 321.999986 161.722033 \nL 329.738044 161.696871 \nL 337.476103 161.631463 \nL 345.214161 161.601277 \nL 352.95222 161.586095 \nL 360.690278 161.596164 \nL 368.428337 161.611336 \nL 376.166395 161.576036 \nL 383.904454 161.581071 \nL 391.642513 161.515663 \nL 399.380571 161.374691 \nL 407.11863 161.0978 \nL 414.856688 160.835993 \nL 422.594747 160.624588 \nL 430.332805 160.53394 \nL 438.070864 160.443292 \nL 445.808922 160.317511 \nL 453.546981 160.337629 \nL 461.285039 160.317511 \nL 469.023098 160.231887 \nL 476.761157 160.136205 \nL 484.499215 160.055704 \nL 492.237274 159.939903 \nL 499.975332 159.949962 \nL 507.713391 159.949962 \nL 515.451449 159.884555 \nL 523.189508 159.713317 \nL 530.927566 159.371009 \nL 538.665625 158.938052 \nL 546.403684 158.55542 \nL 554.141742 158.374202 \nL 561.879801 158.298647 \nL 569.617859 158.283544 \nL 577.355918 158.263426 \nL 585.093976 158.313829 \nL 592.832035 158.313829 \nL 600.570093 158.278529 \nL 608.308152 158.359099 \nL 616.046211 158.490013 \nL 623.784269 158.630975 \nL 631.522328 158.862497 \nL 639.260386 159.104187 \nL 646.998445 159.401195 \nL 654.736503 159.773779 \nL 662.474562 160.010335 \nL 670.21262 160.186607 \nL 677.950679 160.413105 \nL 685.688737 160.690006 \nL 693.426796 160.997083 \nL 701.164855 161.374691 \nL 708.902913 161.807647 \nL 716.640972 162.280859 \nL 724.37903 162.693687 \nL 732.117089 162.910205 \nL 739.855147 162.99574 \nL 747.593206 162.925298 \nL 755.331264 162.970578 \nL 763.069323 162.915229 \nL 770.807382 162.774267 \nL 778.54544 162.587936 \nL 786.283499 162.321193 \nL 794.021557 162.008993 \nL 801.759616 161.716999 \nL 809.497674 161.485467 \nL 817.235733 161.299136 \nL 824.973791 161.133021 \nL 832.71185 160.976965 \nL 840.449909 160.835993 \nL 848.187967 160.740399 \nL 855.926026 160.690006 \nL 863.664084 160.634657 \nL 871.402143 160.564136 \nL 879.140201 160.503753 \nL 886.87826 160.518847 \nL 894.616318 160.538974 \nL 902.354377 160.594402 \nL 910.092435 160.654785 \nL 917.830494 160.820899 \nL 925.568553 161.027358 \nL 933.306611 161.248821 \nL 941.04467 161.515663 \nL 948.782728 161.727067 \nL 956.520787 161.827775 \nL 964.258845 161.90333 \nL 971.996904 161.898295 \nL 979.734962 161.888236 \nL 987.473021 161.827775 \nL 995.21108 161.737126 \n\" clip-path=\"url(#p4053fe7a9f)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 82.12017 160.971931 \nL 89.858229 160.418139 \nL 97.596288 159.612679 \nL 105.334346 158.661162 \nL 113.072405 157.709625 \nL 120.810463 156.808511 \nL 128.548522 156.002972 \nL 136.28658 155.358611 \nL 144.024639 154.855134 \nL 151.762697 154.432246 \nL 159.500756 154.02454 \nL 167.238815 153.626805 \nL 174.976873 153.163662 \nL 182.714932 152.640048 \nL 190.45299 152.21716 \nL 198.191049 151.925166 \nL 205.929107 151.759051 \nL 213.667166 151.708649 \nL 221.405224 151.718796 \nL 229.143283 151.789238 \nL 236.881341 152.015824 \nL 244.6194 152.287602 \nL 252.357459 152.544375 \nL 260.095517 152.786045 \nL 267.833576 152.997449 \nL 275.571634 153.193849 \nL 283.309693 153.400219 \nL 291.047751 153.611702 \nL 298.78581 153.813038 \nL 306.523868 154.054727 \nL 314.261927 154.306466 \nL 321.999986 154.628646 \nL 329.738044 155.021347 \nL 337.476103 155.474421 \nL 345.214161 155.977819 \nL 352.95222 156.551738 \nL 360.690278 157.155844 \nL 368.428337 157.73991 \nL 376.166395 158.404389 \nL 383.904454 159.139408 \nL 391.642513 159.884555 \nL 399.380571 160.564136 \nL 407.11863 161.253856 \nL 414.856688 161.928492 \nL 422.594747 162.587936 \nL 430.332805 163.287724 \nL 438.070864 164.002606 \nL 445.808922 164.752796 \nL 453.546981 165.397157 \nL 461.285039 165.961007 \nL 469.023098 166.368802 \nL 476.761157 166.726203 \nL 484.499215 167.048472 \nL 492.237274 167.370643 \nL 499.975332 167.738113 \nL 507.713391 168.236555 \nL 515.451449 168.951437 \nL 523.189508 169.726701 \nL 530.927566 170.401327 \nL 538.665625 170.874539 \nL 546.403684 171.161499 \nL 554.141742 171.237044 \nL 561.879801 171.146395 \nL 569.617859 170.844343 \nL 577.355918 170.466735 \nL 585.093976 170.04896 \nL 592.832035 169.515297 \nL 600.570093 169.04712 \nL 608.308152 168.649384 \nL 616.046211 168.427832 \nL 623.784269 168.377518 \nL 631.522328 168.055338 \nL 639.260386 166.952789 \nL 646.998445 165.875394 \nL 654.736503 164.440596 \nL 662.474562 162.759174 \nL 670.21262 161.087731 \nL 677.950679 159.386112 \nL 685.688737 157.538476 \nL 693.426796 155.64557 \nL 701.164855 153.843224 \nL 708.902913 152.292715 \nL 716.640972 151.003915 \nL 724.37903 149.981858 \nL 732.117089 149.337497 \nL 739.855147 148.823971 \nL 747.593206 148.401083 \nL 755.331264 148.974982 \nL 763.069323 149.246858 \nL 770.807382 149.433189 \nL 778.54544 149.564004 \nL 786.283499 149.67478 \nL 794.021557 149.780522 \nL 801.759616 149.936588 \nL 809.497674 150.137923 \nL 817.235733 150.36451 \nL 824.973791 150.601145 \nL 832.71185 150.862952 \nL 840.449909 151.175064 \nL 848.187967 151.497244 \nL 855.926026 151.864793 \nL 863.664084 152.26245 \nL 871.402143 152.675269 \nL 879.140201 153.123328 \nL 886.87826 153.611702 \nL 894.616318 154.1151 \nL 902.354377 154.638695 \nL 910.092435 155.187462 \nL 917.830494 155.710978 \nL 925.568553 156.189303 \nL 933.306611 156.592072 \nL 941.04467 157.01496 \nL 948.782728 157.593913 \nL 956.520787 158.333947 \nL 964.258845 159.19986 \nL 971.996904 160.100993 \nL 979.734962 161.087731 \nL 987.473021 162.1097 \nL 995.21108 163.051167 \n\" clip-path=\"url(#p4053fe7a9f)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 82.12017 158.399355 \nL 89.858229 158.515165 \nL 97.596288 158.761869 \nL 105.334346 159.084049 \nL 113.072405 159.391146 \nL 120.810463 159.68313 \nL 128.548522 159.9248 \nL 136.28658 160.13118 \nL 144.024639 160.307364 \nL 151.762697 160.468532 \nL 159.500756 160.64975 \nL 167.238815 160.861155 \nL 174.976873 161.092766 \nL 182.714932 161.399853 \nL 190.45299 161.852937 \nL 198.191049 162.436915 \nL 205.929107 163.126634 \nL 213.667166 163.786177 \nL 221.405224 164.36512 \nL 229.143283 164.823238 \nL 236.881341 165.105164 \nL 244.6194 165.246126 \nL 252.357459 165.301475 \nL 260.095517 165.326715 \nL 267.833576 165.341809 \nL 275.571634 165.341809 \nL 283.309693 165.346843 \nL 291.047751 165.301475 \nL 298.78581 165.231033 \nL 306.523868 165.13535 \nL 314.261927 165.01955 \nL 321.999986 164.81317 \nL 329.738044 164.536279 \nL 337.476103 164.178878 \nL 345.214161 163.776108 \nL 352.95222 163.413593 \nL 360.690278 163.096447 \nL 368.428337 162.814522 \nL 376.166395 162.552715 \nL 383.904454 162.326217 \nL 391.642513 162.134862 \nL 399.380571 161.963703 \nL 407.11863 161.762367 \nL 414.856688 161.525722 \nL 422.594747 161.223669 \nL 430.332805 160.790713 \nL 438.070864 160.241956 \nL 445.808922 159.657968 \nL 453.546981 159.124305 \nL 461.285039 158.706451 \nL 469.023098 158.384271 \nL 476.761157 158.107361 \nL 484.499215 157.885907 \nL 492.237274 157.684473 \nL 499.975332 157.513324 \nL 507.713391 157.372361 \nL 515.451449 157.286738 \nL 523.189508 157.251517 \nL 530.927566 157.22133 \nL 538.665625 157.186109 \nL 546.403684 157.120623 \nL 554.141742 157.110554 \nL 561.879801 157.160957 \nL 569.617859 157.271654 \nL 577.355918 157.362293 \nL 585.093976 157.332106 \nL 592.832035 157.165991 \nL 600.570093 156.879032 \nL 608.308152 156.546704 \nL 616.046211 156.214455 \nL 623.784269 155.922481 \nL 631.522328 155.731194 \nL 639.260386 155.539829 \nL 646.998445 155.353576 \nL 654.736503 155.278022 \nL 662.474562 155.28809 \nL 670.21262 155.444235 \nL 677.950679 155.736228 \nL 685.688737 156.078527 \nL 693.426796 156.481296 \nL 701.164855 156.904184 \nL 708.902913 157.352244 \nL 716.640972 157.865691 \nL 724.37903 158.43961 \nL 732.117089 159.038779 \nL 739.855147 159.68313 \nL 747.593206 160.332604 \nL 755.331264 161.158173 \nL 763.069323 161.958678 \nL 770.807382 162.71884 \nL 778.54544 163.433721 \nL 786.283499 164.103323 \nL 794.021557 164.722521 \nL 801.759616 165.311534 \nL 809.497674 165.850232 \nL 817.235733 166.318409 \nL 824.973791 166.685958 \nL 832.71185 166.947755 \nL 840.449909 167.108845 \nL 848.187967 167.204528 \nL 855.926026 167.219621 \nL 863.664084 167.118904 \nL 871.402143 166.882269 \nL 879.140201 166.555054 \nL 886.87826 166.21778 \nL 894.616318 165.850232 \nL 902.354377 165.417275 \nL 910.092435 164.913887 \nL 917.830494 164.415434 \nL 925.568553 163.881771 \nL 933.306611 163.348186 \nL 941.04467 162.819557 \nL 948.782728 162.280859 \nL 956.520787 161.817716 \nL 964.258845 161.470373 \nL 971.996904 161.183414 \nL 979.734962 160.931675 \nL 987.473021 160.750457 \nL 995.21108 160.679937 \n\" clip-path=\"url(#p4053fe7a9f)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 203.100734 \nL 36.465625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 1040.865625 203.100734 \nL 1040.865625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 203.100734 \nL 1040.865625 203.100734 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 22.318125 \nL 1040.865625 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_14\">\n    <!-- Sample 78 Class:['ArrestoMomentumCharm'] -->\n    <g transform=\"translate(403.233438 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-53\" d=\"M 3425 4513 \nL 3425 3897 \nQ 3066 4069 2747 4153 \nQ 2428 4238 2131 4238 \nQ 1616 4238 1336 4038 \nQ 1056 3838 1056 3469 \nQ 1056 3159 1242 3001 \nQ 1428 2844 1947 2747 \nL 2328 2669 \nQ 3034 2534 3370 2195 \nQ 3706 1856 3706 1288 \nQ 3706 609 3251 259 \nQ 2797 -91 1919 -91 \nQ 1588 -91 1214 -16 \nQ 841 59 441 206 \nL 441 856 \nQ 825 641 1194 531 \nQ 1563 422 1919 422 \nQ 2459 422 2753 634 \nQ 3047 847 3047 1241 \nQ 3047 1584 2836 1778 \nQ 2625 1972 2144 2069 \nL 1759 2144 \nQ 1053 2284 737 2584 \nQ 422 2884 422 3419 \nQ 422 4038 858 4394 \nQ 1294 4750 2059 4750 \nQ 2388 4750 2728 4690 \nQ 3069 4631 3425 4513 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-3a\" d=\"M 750 794 \nL 1409 794 \nL 1409 0 \nL 750 0 \nL 750 794 \nz\nM 750 3309 \nL 1409 3309 \nL 1409 2516 \nL 750 2516 \nL 750 3309 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-5b\" d=\"M 550 4863 \nL 1875 4863 \nL 1875 4416 \nL 1125 4416 \nL 1125 -397 \nL 1875 -397 \nL 1875 -844 \nL 550 -844 \nL 550 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-27\" d=\"M 1147 4666 \nL 1147 2931 \nL 616 2931 \nL 616 4666 \nL 1147 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-41\" d=\"M 2188 4044 \nL 1331 1722 \nL 3047 1722 \nL 2188 4044 \nz\nM 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 3669 0 \nL 3244 1197 \nL 1141 1197 \nL 716 0 \nL 50 0 \nL 1831 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-5d\" d=\"M 1947 4863 \nL 1947 -844 \nL 622 -844 \nL 622 -397 \nL 1369 -397 \nL 1369 4416 \nL 622 4416 \nL 622 4863 \nL 1947 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-53\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"124.755859\"/>\n     <use xlink:href=\"#DejaVuSans-70\" x=\"222.167969\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"285.644531\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"313.427734\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"374.951172\"/>\n     <use xlink:href=\"#DejaVuSans-37\" x=\"406.738281\"/>\n     <use xlink:href=\"#DejaVuSans-38\" x=\"470.361328\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"533.984375\"/>\n     <use xlink:href=\"#DejaVuSans-43\" x=\"565.771484\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"635.595703\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"663.378906\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"724.658203\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"776.757812\"/>\n     <use xlink:href=\"#DejaVuSans-3a\" x=\"828.857422\"/>\n     <use xlink:href=\"#DejaVuSans-5b\" x=\"862.548828\"/>\n     <use xlink:href=\"#DejaVuSans-27\" x=\"901.5625\"/>\n     <use xlink:href=\"#DejaVuSans-41\" x=\"929.052734\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"997.460938\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"1036.824219\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1075.6875\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"1137.210938\"/>\n     <use xlink:href=\"#DejaVuSans-74\" x=\"1189.310547\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1228.519531\"/>\n     <use xlink:href=\"#DejaVuSans-4d\" x=\"1289.701172\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1375.980469\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1437.162109\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1534.574219\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"1596.097656\"/>\n     <use xlink:href=\"#DejaVuSans-74\" x=\"1659.476562\"/>\n     <use xlink:href=\"#DejaVuSans-75\" x=\"1698.685547\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1762.064453\"/>\n     <use xlink:href=\"#DejaVuSans-43\" x=\"1859.476562\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"1929.300781\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"1992.679688\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"2053.958984\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"2093.322266\"/>\n     <use xlink:href=\"#DejaVuSans-27\" x=\"2190.734375\"/>\n     <use xlink:href=\"#DejaVuSans-5d\" x=\"2218.224609\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 988.667188 118.386875 \nL 1033.865625 118.386875 \nQ 1035.865625 118.386875 1035.865625 116.386875 \nL 1035.865625 29.318125 \nQ 1035.865625 27.318125 1033.865625 27.318125 \nL 988.667188 27.318125 \nQ 986.667188 27.318125 986.667188 29.318125 \nL 986.667188 116.386875 \nQ 986.667188 118.386875 988.667188 118.386875 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 990.667188 35.416562 \nL 1000.667188 35.416562 \nL 1010.667188 35.416562 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- aX -->\n     <g transform=\"translate(1018.667188 38.916562) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-58\" d=\"M 403 4666 \nL 1081 4666 \nL 2241 2931 \nL 3406 4666 \nL 4084 4666 \nL 2584 2425 \nL 4184 0 \nL 3506 0 \nL 2194 1984 \nL 872 0 \nL 191 0 \nL 1856 2491 \nL 403 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-58\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 990.667188 50.094687 \nL 1000.667188 50.094687 \nL 1010.667188 50.094687 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_16\">\n     <!-- aY -->\n     <g transform=\"translate(1018.667188 53.594687) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-59\" d=\"M -13 4666 \nL 666 4666 \nL 1959 2747 \nL 3244 4666 \nL 3922 4666 \nL 2272 2222 \nL 2272 0 \nL 1638 0 \nL 1638 2222 \nL -13 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-59\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 990.667188 64.772812 \nL 1000.667188 64.772812 \nL 1010.667188 64.772812 \n\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- aZ -->\n     <g transform=\"translate(1018.667188 68.272812) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-5a\" d=\"M 359 4666 \nL 4025 4666 \nL 4025 4184 \nL 1075 531 \nL 4097 531 \nL 4097 0 \nL 288 0 \nL 288 481 \nL 3238 4134 \nL 359 4134 \nL 359 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-5a\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 990.667188 79.450937 \nL 1000.667188 79.450937 \nL 1010.667188 79.450937 \n\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_18\">\n     <!-- gX -->\n     <g transform=\"translate(1018.667188 82.950937) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \nQ 2906 2416 2648 2759 \nQ 2391 3103 1925 3103 \nQ 1463 3103 1205 2759 \nQ 947 2416 947 1791 \nQ 947 1169 1205 825 \nQ 1463 481 1925 481 \nQ 2391 481 2648 825 \nQ 2906 1169 2906 1791 \nz\nM 3481 434 \nQ 3481 -459 3084 -895 \nQ 2688 -1331 1869 -1331 \nQ 1566 -1331 1297 -1286 \nQ 1028 -1241 775 -1147 \nL 775 -588 \nQ 1028 -725 1275 -790 \nQ 1522 -856 1778 -856 \nQ 2344 -856 2625 -561 \nQ 2906 -266 2906 331 \nL 2906 616 \nQ 2728 306 2450 153 \nQ 2172 0 1784 0 \nQ 1141 0 747 490 \nQ 353 981 353 1791 \nQ 353 2603 747 3093 \nQ 1141 3584 1784 3584 \nQ 2172 3584 2450 3431 \nQ 2728 3278 2906 2969 \nL 2906 3500 \nL 3481 3500 \nL 3481 434 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-58\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 990.667188 94.129062 \nL 1000.667188 94.129062 \nL 1010.667188 94.129062 \n\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- gY -->\n     <g transform=\"translate(1018.667188 97.629062) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-59\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_25\">\n     <path d=\"M 990.667188 108.807187 \nL 1000.667188 108.807187 \nL 1010.667188 108.807187 \n\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_20\">\n     <!-- gZ -->\n     <g transform=\"translate(1018.667188 112.307187) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-5a\" x=\"63.476562\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 36.465625 420.039864 \nL 1040.865625 420.039864 \nL 1040.865625 239.257255 \nL 36.465625 239.257255 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_8\">\n     <g id=\"line2d_26\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"82.12017\" y=\"420.039864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 0 -->\n      <g transform=\"translate(78.93892 434.638302) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_27\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"236.881341\" y=\"420.039864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 20 -->\n      <g transform=\"translate(230.518841 434.638302) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_28\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"391.642513\" y=\"420.039864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 40 -->\n      <g transform=\"translate(385.280013 434.638302) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_29\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"546.403684\" y=\"420.039864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 60 -->\n      <g transform=\"translate(540.041184 434.638302) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_30\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"701.164855\" y=\"420.039864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 80 -->\n      <g transform=\"translate(694.802355 434.638302) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_31\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"855.926026\" y=\"420.039864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 100 -->\n      <g transform=\"translate(846.382276 434.638302) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_32\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"1010.687197\" y=\"420.039864\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 120 -->\n      <g transform=\"translate(1001.143447 434.638302) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_33\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"411.540681\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0.3 -->\n      <g transform=\"translate(13.5625 415.3399) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_34\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"386.492431\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 0.4 -->\n      <g transform=\"translate(13.5625 390.29165) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_35\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"361.444181\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 0.5 -->\n      <g transform=\"translate(13.5625 365.243399) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_36\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"336.395931\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 0.6 -->\n      <g transform=\"translate(13.5625 340.195149) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_37\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"311.34768\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 0.7 -->\n      <g transform=\"translate(13.5625 315.146899) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_38\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"286.29943\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_33\">\n      <!-- 0.8 -->\n      <g transform=\"translate(13.5625 290.098649) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_39\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"261.25118\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_34\">\n      <!-- 0.9 -->\n      <g transform=\"translate(13.5625 265.050399) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_40\">\n    <path d=\"M 82.12017 330.071252 \nL 89.858229 326.908901 \nL 97.596288 323.33953 \nL 105.334346 319.613604 \nL 113.072405 316.451268 \nL 120.810463 314.290849 \nL 128.548522 312.881882 \nL 136.28658 312.537479 \nL 144.024639 314.165601 \nL 151.762697 316.639116 \nL 159.500756 320.584228 \nL 167.238815 324.278846 \nL 174.976873 328.850149 \nL 182.714932 332.701307 \nL 190.45299 335.581856 \nL 198.191049 337.460488 \nL 205.929107 338.775515 \nL 213.667166 339.495645 \nL 221.405224 340.090542 \nL 229.143283 341.186414 \nL 236.881341 342.313578 \nL 244.6194 343.910407 \nL 252.357459 345.820332 \nL 260.095517 348.48171 \nL 267.833576 351.58143 \nL 275.571634 354.994261 \nL 283.309693 358.407077 \nL 291.047751 362.007763 \nL 298.78581 365.890244 \nL 306.523868 369.303067 \nL 314.261927 372.465411 \nL 321.999986 375.909542 \nL 329.738044 378.727475 \nL 337.476103 380.919195 \nL 345.214161 382.609956 \nL 352.95222 383.424017 \nL 360.690278 383.768436 \nL 368.428337 383.549264 \nL 376.166395 383.549264 \nL 383.904454 382.547332 \nL 391.642513 381.514092 \nL 399.380571 379.698091 \nL 407.11863 377.568995 \nL 414.856688 375.126789 \nL 422.594747 372.121 \nL 430.332805 369.522238 \nL 438.070864 366.547765 \nL 445.808922 363.823764 \nL 453.546981 361.632044 \nL 461.285039 359.816043 \nL 469.023098 357.248604 \nL 476.761157 354.368055 \nL 484.499215 351.362259 \nL 492.237274 348.231231 \nL 499.975332 344.849708 \nL 507.713391 340.68544 \nL 515.451449 335.613179 \nL 523.189508 331.292355 \nL 530.927566 327.785601 \nL 538.665625 323.997044 \nL 546.403684 321.523529 \nL 554.141742 320.365057 \nL 561.879801 320.051947 \nL 569.617859 320.803399 \nL 577.355918 322.77594 \nL 585.093976 325.593873 \nL 592.832035 328.317867 \nL 600.570093 330.791382 \nL 608.308152 332.012485 \nL 616.046211 333.609315 \nL 623.784269 334.454692 \nL 631.522328 335.362685 \nL 639.260386 336.270692 \nL 646.998445 336.677727 \nL 654.736503 337.397857 \nL 662.474562 338.274557 \nL 670.21262 338.994686 \nL 677.950679 339.871371 \nL 685.688737 341.436893 \nL 693.426796 343.065031 \nL 701.164855 344.599229 \nL 708.902913 345.976888 \nL 716.640972 347.323223 \nL 724.37903 348.231231 \nL 732.117089 348.575649 \nL 739.855147 348.199923 \nL 747.593206 347.886813 \nL 755.331264 347.197991 \nL 763.069323 346.321306 \nL 770.807382 345.475914 \nL 778.54544 344.505305 \nL 786.283499 344.098271 \nL 794.021557 343.879099 \nL 801.759616 343.158955 \nL 809.497674 342.470133 \nL 817.235733 341.155106 \nL 824.973791 338.368481 \nL 832.71185 335.112205 \nL 840.449909 332.388211 \nL 848.187967 330.415655 \nL 855.926026 329.78945 \nL 863.664084 329.758142 \nL 871.402143 329.319799 \nL 879.140201 328.630977 \nL 886.87826 327.409874 \nL 894.616318 326.814977 \nL 902.354377 325.844353 \nL 910.092435 324.748496 \nL 917.830494 324.247523 \nL 925.568553 324.090983 \nL 933.306611 324.122291 \nL 941.04467 324.310154 \nL 948.782728 323.90312 \nL 956.520787 323.621317 \nL 964.258845 323.777872 \nL 971.996904 323.746564 \nL 979.734962 323.80918 \nL 987.473021 323.527393 \nL 995.21108 323.590009 \n\" clip-path=\"url(#p30dc3d505c)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path d=\"M 82.12017 367.862792 \nL 89.858229 370.492862 \nL 97.596288 374.844994 \nL 105.334346 380.136434 \nL 113.072405 384.425949 \nL 120.810463 384.081538 \nL 128.548522 378.696167 \nL 136.28658 370.117136 \nL 144.024639 358.814111 \nL 151.762697 345.225435 \nL 159.500756 332.075101 \nL 167.238815 317.860219 \nL 174.976873 304.490729 \nL 182.714932 292.592807 \nL 190.45299 284.702598 \nL 198.191049 281.195843 \nL 205.929107 279.285918 \nL 213.667166 281.915988 \nL 221.405224 288.146736 \nL 229.143283 296.694459 \nL 236.881341 303.864509 \nL 244.6194 308.435826 \nL 252.357459 311.253744 \nL 260.095517 311.817334 \nL 267.833576 310.032641 \nL 275.571634 305.868373 \nL 283.309693 301.359686 \nL 291.047751 299.606316 \nL 298.78581 302.486865 \nL 306.523868 308.279271 \nL 314.261927 316.01291 \nL 321.999986 324.905052 \nL 329.738044 336.114137 \nL 337.476103 348.951361 \nL 345.214161 362.477421 \nL 352.95222 374.469267 \nL 360.690278 383.956299 \nL 368.428337 390.875876 \nL 376.166395 393.098904 \nL 383.904454 394.2887 \nL 391.642513 395.384563 \nL 399.380571 395.290632 \nL 407.11863 393.380699 \nL 414.856688 390.656705 \nL 422.594747 388.30843 \nL 430.332805 387.619601 \nL 438.070864 388.840704 \nL 445.808922 390.218355 \nL 453.546981 392.660562 \nL 461.285039 396.35518 \nL 469.023098 397.26318 \nL 476.761157 395.541111 \nL 484.499215 393.192836 \nL 492.237274 389.905252 \nL 499.975332 384.582504 \nL 507.713391 377.349816 \nL 515.451449 369.052588 \nL 523.189508 363.949003 \nL 530.927566 359.440317 \nL 538.665625 355.24474 \nL 546.403684 353.053013 \nL 554.141742 355.432603 \nL 561.879801 361.444181 \nL 569.617859 369.240451 \nL 577.355918 374.782378 \nL 585.093976 375.001549 \nL 592.832035 370.367623 \nL 600.570093 362.039078 \nL 608.308152 352.458115 \nL 616.046211 341.937851 \nL 623.784269 332.576075 \nL 631.522328 326.564497 \nL 639.260386 324.967668 \nL 646.998445 325.750428 \nL 654.736503 326.533189 \nL 662.474562 328.161312 \nL 670.21262 330.509594 \nL 677.950679 333.484068 \nL 685.688737 338.900762 \nL 693.426796 346.133443 \nL 701.164855 352.458115 \nL 708.902913 358.34446 \nL 716.640972 364.387353 \nL 724.37903 369.741417 \nL 732.117089 372.747206 \nL 739.855147 372.340171 \nL 747.593206 372.24624 \nL 755.331264 376.347892 \nL 763.069323 384.425949 \nL 770.807382 392.253527 \nL 778.54544 394.351323 \nL 786.283499 390.844561 \nL 794.021557 383.298778 \nL 801.759616 373.342103 \nL 809.497674 361.882523 \nL 817.235733 349.702813 \nL 824.973791 339.526968 \nL 832.71185 332.106424 \nL 840.449909 326.251387 \nL 848.187967 321.930563 \nL 855.926026 320.083255 \nL 863.664084 322.869879 \nL 871.402143 329.163244 \nL 879.140201 335.425316 \nL 886.87826 340.528885 \nL 894.616318 344.943647 \nL 902.354377 349.013992 \nL 910.092435 354.242808 \nL 917.830494 359.315084 \nL 925.568553 361.882523 \nL 933.306611 362.665284 \nL 941.04467 363.510661 \nL 948.782728 363.322798 \nL 956.520787 361.225009 \nL 964.258845 358.657556 \nL 971.996904 355.150801 \nL 979.734962 351.800602 \nL 987.473021 348.48171 \nL 995.21108 345.632469 \n\" clip-path=\"url(#p30dc3d505c)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_42\">\n    <path d=\"M 82.12017 307.246031 \nL 89.858229 301.109207 \nL 97.596288 296.60052 \nL 105.334346 291.622183 \nL 113.072405 285.516666 \nL 120.810463 278.283986 \nL 128.548522 270.800826 \nL 136.28658 262.159179 \nL 144.024639 253.830642 \nL 151.762697 247.474647 \nL 159.500756 249.353264 \nL 167.238815 255.803183 \nL 174.976873 267.450627 \nL 182.714932 282.542193 \nL 190.45299 299.856795 \nL 198.191049 318.95609 \nL 205.929107 337.209993 \nL 213.667166 352.677287 \nL 221.405224 362.571352 \nL 229.143283 368.864724 \nL 236.881341 368.395067 \nL 244.6194 366.359902 \nL 252.357459 362.665284 \nL 260.095517 356.935494 \nL 267.833576 351.612738 \nL 275.571634 349.796738 \nL 283.309693 350.736054 \nL 291.047751 353.679219 \nL 298.78581 359.565564 \nL 306.523868 366.641689 \nL 314.261927 373.592582 \nL 321.999986 380.167749 \nL 329.738044 382.516024 \nL 337.476103 382.359469 \nL 345.214161 381.263605 \nL 352.95222 378.383056 \nL 360.690278 374.594514 \nL 368.428337 369.522238 \nL 376.166395 364.700456 \nL 383.904454 361.350257 \nL 391.642513 361.256318 \nL 399.380571 366.172039 \nL 407.11863 373.780446 \nL 414.856688 382.202921 \nL 422.594747 391.65863 \nL 430.332805 401.490072 \nL 438.070864 407.219855 \nL 445.808922 409.004547 \nL 453.546981 408.315718 \nL 461.285039 404.464553 \nL 469.023098 400.268969 \nL 476.761157 395.948145 \nL 484.499215 388.965944 \nL 492.237274 376.535755 \nL 499.975332 358.876742 \nL 507.713391 336.364617 \nL 515.451449 310.658847 \nL 523.189508 289.086052 \nL 530.927566 271.239169 \nL 538.665625 256.398081 \nL 546.403684 250.167333 \nL 554.141742 254.362909 \nL 561.879801 266.448695 \nL 569.617859 282.35433 \nL 577.355918 297.696391 \nL 585.093976 312.725327 \nL 592.832035 326.564497 \nL 600.570093 345.6951 \nL 608.308152 359.033282 \nL 616.046211 372.684582 \nL 623.784269 381.858503 \nL 631.522328 389.780013 \nL 639.260386 397.513659 \nL 646.998445 404.74634 \nL 654.736503 410.225651 \nL 662.474562 411.822473 \nL 670.21262 409.693369 \nL 677.950679 406.81282 \nL 685.688737 401.458764 \nL 693.426796 393.662494 \nL 701.164855 384.457257 \nL 708.902913 376.3792 \nL 716.640972 372.089684 \nL 724.37903 370.273691 \nL 732.117089 369.083896 \nL 739.855147 366.26597 \nL 747.593206 362.633976 \nL 755.331264 359.158529 \nL 763.069323 355.589159 \nL 770.807382 353.867082 \nL 778.54544 355.620467 \nL 786.283499 358.970666 \nL 794.021557 362.226942 \nL 801.759616 365.451901 \nL 809.497674 366.829552 \nL 817.235733 368.081964 \nL 824.973791 369.553554 \nL 832.71185 370.586794 \nL 840.449909 370.367623 \nL 848.187967 366.704313 \nL 855.926026 360.066522 \nL 863.664084 352.802534 \nL 871.402143 347.511086 \nL 879.140201 345.006264 \nL 886.87826 343.597297 \nL 894.616318 344.286134 \nL 902.354377 342.657996 \nL 910.092435 339.777447 \nL 917.830494 336.020213 \nL 925.568553 332.73263 \nL 933.306611 330.572211 \nL 941.04467 330.979245 \nL 948.782728 333.609315 \nL 956.520787 336.458556 \nL 964.258845 337.209993 \nL 971.996904 335.487932 \nL 979.734962 334.266828 \nL 987.473021 335.206144 \nL 995.21108 337.867522 \n\" clip-path=\"url(#p30dc3d505c)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_43\">\n    <path d=\"M 82.12017 356.73924 \nL 89.858229 356.643689 \nL 97.596288 356.750706 \nL 105.334346 356.915025 \nL 113.072405 357.002932 \nL 120.810463 356.918832 \nL 128.548522 356.811815 \nL 136.28658 357.006755 \nL 144.024639 357.411908 \nL 151.762697 357.901102 \nL 159.500756 358.432383 \nL 167.238815 358.864276 \nL 174.976873 359.804517 \nL 182.714932 360.977904 \nL 190.45299 361.918094 \nL 198.191049 362.713127 \nL 205.929107 363.324619 \nL 213.667166 363.676293 \nL 221.405224 363.966725 \nL 229.143283 364.123467 \nL 236.881341 363.959089 \nL 244.6194 363.473717 \nL 252.357459 363.229053 \nL 260.095517 363.420178 \nL 267.833576 363.897973 \nL 275.571634 364.413899 \nL 283.309693 364.87642 \nL 291.047751 365.270048 \nL 298.78581 365.549088 \nL 306.523868 365.407627 \nL 314.261927 365.094278 \nL 321.999986 364.70822 \nL 329.738044 364.119645 \nL 337.476103 363.316975 \nL 345.214161 362.575488 \nL 352.95222 362.204771 \nL 360.690278 362.265953 \nL 368.428337 362.682506 \nL 376.166395 363.175581 \nL 383.904454 363.932349 \nL 391.642513 364.906974 \nL 399.380571 365.870141 \nL 407.11863 366.661292 \nL 414.856688 367.253682 \nL 422.594747 367.582379 \nL 430.332805 367.781141 \nL 438.070864 367.762038 \nL 445.808922 367.349241 \nL 453.546981 367.012907 \nL 461.285039 366.840951 \nL 469.023098 366.623094 \nL 476.761157 366.198838 \nL 484.499215 365.705762 \nL 492.237274 364.968097 \nL 499.975332 363.859708 \nL 507.713391 362.579303 \nL 515.451449 361.348614 \nL 523.189508 360.695057 \nL 530.927566 360.167598 \nL 538.665625 359.701322 \nL 546.403684 359.41083 \nL 554.141742 359.257963 \nL 561.879801 359.154767 \nL 569.617859 359.009559 \nL 577.355918 359.055393 \nL 585.093976 359.330612 \nL 592.832035 359.579031 \nL 600.570093 359.838901 \nL 608.308152 360.247876 \nL 616.046211 360.828814 \nL 623.784269 361.38682 \nL 631.522328 362.048096 \nL 639.260386 362.697846 \nL 646.998445 363.14502 \nL 654.736503 363.370514 \nL 662.474562 363.328434 \nL 670.21262 363.141198 \nL 677.950679 362.850706 \nL 685.688737 362.674862 \nL 693.426796 362.655766 \nL 701.164855 362.606109 \nL 708.902913 362.674862 \nL 716.640972 362.885082 \nL 724.37903 363.14502 \nL 732.117089 363.362877 \nL 739.855147 363.86353 \nL 747.593206 364.417721 \nL 755.331264 364.991074 \nL 763.069323 365.449707 \nL 770.807382 365.606389 \nL 778.54544 365.426789 \nL 786.283499 365.193591 \nL 794.021557 365.025451 \nL 801.759616 365.021636 \nL 809.497674 365.017814 \nL 817.235733 364.987252 \nL 824.973791 364.975734 \nL 832.71185 365.025451 \nL 840.449909 364.933714 \nL 848.187967 364.704398 \nL 855.926026 364.287786 \nL 863.664084 363.741231 \nL 871.402143 363.294058 \nL 879.140201 362.865979 \nL 886.87826 362.594651 \nL 894.616318 362.430265 \nL 902.354377 362.395889 \nL 910.092435 362.45319 \nL 917.830494 362.491447 \nL 925.568553 362.434087 \nL 933.306611 362.246791 \nL 941.04467 362.124492 \nL 948.782728 362.03657 \nL 956.520787 361.944893 \nL 964.258845 362.021289 \nL 971.996904 362.082472 \nL 979.734962 362.009831 \nL 987.473021 361.837875 \nL 995.21108 361.593219 \n\" clip-path=\"url(#p30dc3d505c)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_44\">\n    <path d=\"M 82.12017 366.233221 \nL 89.858229 365.652283 \nL 97.596288 364.929892 \nL 105.334346 364.05847 \nL 113.072405 363.27114 \nL 120.810463 362.388252 \nL 128.548522 361.444181 \nL 136.28658 360.393099 \nL 144.024639 359.147138 \nL 151.762697 357.354547 \nL 159.500756 355.065148 \nL 167.238815 353.020391 \nL 174.976873 351.07877 \nL 182.714932 349.236537 \nL 190.45299 347.765014 \nL 198.191049 346.675727 \nL 205.929107 345.792846 \nL 213.667166 345.682007 \nL 221.405224 346.618441 \nL 229.143283 347.199364 \nL 236.881341 348.25809 \nL 244.6194 349.163903 \nL 252.357459 349.924493 \nL 260.095517 350.604805 \nL 267.833576 351.074963 \nL 275.571634 351.327189 \nL 283.309693 351.522129 \nL 291.047751 351.755274 \nL 298.78581 352.049588 \nL 306.523868 352.515864 \nL 314.261927 353.119705 \nL 321.999986 353.83446 \nL 329.738044 354.916103 \nL 337.476103 356.097134 \nL 345.214161 357.132883 \nL 352.95222 358.042563 \nL 360.690278 358.787879 \nL 368.428337 359.345885 \nL 376.166395 359.731883 \nL 383.904454 359.877166 \nL 391.642513 359.86564 \nL 399.380571 359.735705 \nL 407.11863 359.720417 \nL 414.856688 359.934467 \nL 422.594747 360.316703 \nL 430.332805 360.622408 \nL 438.070864 361.520637 \nL 445.808922 363.133561 \nL 453.546981 364.368064 \nL 461.285039 365.610203 \nL 469.023098 366.695668 \nL 476.761157 367.666479 \nL 484.499215 368.629645 \nL 492.237274 369.577539 \nL 499.975332 370.422229 \nL 507.713391 371.136917 \nL 515.451449 371.809584 \nL 523.189508 371.194277 \nL 530.927566 369.688371 \nL 538.665625 368.518806 \nL 546.403684 366.294397 \nL 554.141742 363.527196 \nL 561.879801 361.222501 \nL 569.617859 359.219772 \nL 577.355918 357.671793 \nL 585.093976 356.39527 \nL 592.832035 355.325078 \nL 600.570093 354.059961 \nL 608.308152 353.949122 \nL 616.046211 354.071413 \nL 623.784269 354.300736 \nL 631.522328 354.851113 \nL 639.260386 355.256266 \nL 646.998445 355.925112 \nL 654.736503 356.872997 \nL 662.474562 358.164868 \nL 670.21262 359.556113 \nL 677.950679 360.740952 \nL 685.688737 361.788159 \nL 693.426796 362.693964 \nL 701.164855 363.485176 \nL 708.902913 364.008805 \nL 716.640972 364.203685 \nL 724.37903 364.2916 \nL 732.117089 364.3757 \nL 739.855147 364.478904 \nL 747.593206 364.563004 \nL 755.331264 364.59738 \nL 763.069323 364.612661 \nL 770.807382 364.543901 \nL 778.54544 364.371886 \nL 786.283499 364.280142 \nL 794.021557 364.314525 \nL 801.759616 364.433002 \nL 809.497674 364.696761 \nL 817.235733 364.964275 \nL 824.973791 365.262411 \nL 832.71185 365.656105 \nL 840.449909 366.156758 \nL 848.187967 366.764495 \nL 855.926026 367.356885 \nL 863.664084 367.773497 \nL 871.402143 367.93406 \nL 879.140201 367.91878 \nL 886.87826 367.869056 \nL 894.616318 367.73912 \nL 902.354377 367.72384 \nL 910.092435 367.700922 \nL 917.830494 367.616822 \nL 925.568553 367.452444 \nL 933.306611 367.15819 \nL 941.04467 366.714771 \nL 948.782728 366.256139 \nL 956.520787 365.946545 \nL 964.258845 365.747842 \nL 971.996904 365.621662 \nL 979.734962 365.426789 \nL 987.473021 365.155393 \nL 995.21108 364.880235 \n\" clip-path=\"url(#p30dc3d505c)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_45\">\n    <path d=\"M 82.12017 356.953276 \nL 89.858229 356.915025 \nL 97.596288 356.800363 \nL 105.334346 356.532849 \nL 113.072405 356.08949 \nL 120.810463 355.382379 \nL 128.548522 354.549215 \nL 136.28658 353.792447 \nL 144.024639 353.154148 \nL 151.762697 352.706982 \nL 159.500756 352.554055 \nL 167.238815 352.50822 \nL 174.976873 352.867479 \nL 182.714932 353.586041 \nL 190.45299 354.59505 \nL 198.191049 355.867811 \nL 205.929107 357.2399 \nL 213.667166 358.73434 \nL 221.405224 360.140858 \nL 229.143283 361.375354 \nL 236.881341 362.311788 \nL 244.6194 363.03418 \nL 252.357459 363.64185 \nL 260.095517 364.2075 \nL 267.833576 364.773217 \nL 275.571634 365.388531 \nL 283.309693 366.156758 \nL 291.047751 367.100829 \nL 298.78581 368.171013 \nL 306.523868 369.210576 \nL 314.261927 370.120271 \nL 321.999986 370.915237 \nL 329.738044 371.507694 \nL 337.476103 371.84785 \nL 345.214161 371.874582 \nL 352.95222 371.595549 \nL 360.690278 371.037536 \nL 368.428337 370.265487 \nL 376.166395 369.332942 \nL 383.904454 368.358309 \nL 391.642513 367.433341 \nL 399.380571 366.50844 \nL 407.11863 365.587286 \nL 414.856688 364.734959 \nL 422.594747 363.993465 \nL 430.332805 363.362877 \nL 438.070864 362.831603 \nL 445.808922 362.285049 \nL 453.546981 361.761419 \nL 461.285039 361.268336 \nL 469.023098 360.633934 \nL 476.761157 359.938274 \nL 484.499215 359.322908 \nL 492.237274 358.768709 \nL 499.975332 358.26042 \nL 507.713391 357.855267 \nL 515.451449 357.583871 \nL 523.189508 357.698592 \nL 530.927566 357.939367 \nL 538.665625 358.222155 \nL 546.403684 358.619679 \nL 554.141742 359.063038 \nL 561.879801 359.418459 \nL 569.617859 359.548469 \nL 577.355918 359.429925 \nL 585.093976 359.055393 \nL 592.832035 358.585295 \nL 600.570093 358.14958 \nL 608.308152 357.882006 \nL 616.046211 357.855267 \nL 623.784269 358.099864 \nL 631.522328 358.65024 \nL 639.260386 359.361173 \nL 646.998445 360.14468 \nL 654.736503 360.851791 \nL 662.474562 361.513 \nL 670.21262 362.113034 \nL 677.950679 362.667225 \nL 685.688737 363.198499 \nL 693.426796 363.634213 \nL 701.164855 363.886447 \nL 708.902913 363.966725 \nL 716.640972 363.913246 \nL 724.37903 363.729772 \nL 732.117089 363.378158 \nL 739.855147 362.972997 \nL 747.593206 362.583125 \nL 755.331264 362.27359 \nL 763.069323 361.956352 \nL 770.807382 361.448003 \nL 778.54544 360.580395 \nL 786.283499 359.579031 \nL 794.021557 358.673217 \nL 801.759616 357.958462 \nL 809.497674 357.423374 \nL 817.235733 357.13669 \nL 824.973791 357.178777 \nL 832.71185 357.496008 \nL 840.449909 357.98138 \nL 848.187967 358.570022 \nL 855.926026 359.280895 \nL 863.664084 360.037663 \nL 871.402143 360.775335 \nL 879.140201 361.318068 \nL 886.87826 361.696414 \nL 894.616318 361.933375 \nL 902.354377 362.128314 \nL 910.092435 362.25825 \nL 917.830494 362.281234 \nL 925.568553 362.135951 \nL 933.306611 361.914272 \nL 941.04467 361.73468 \nL 948.782728 361.55502 \nL 956.520787 361.337163 \nL 964.258845 361.184311 \nL 971.996904 361.096388 \nL 979.734962 361.0811 \nL 987.473021 361.146045 \nL 995.21108 361.283684 \n\" clip-path=\"url(#p30dc3d505c)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 36.465625 420.039864 \nL 36.465625 239.257255 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 1040.865625 420.039864 \nL 1040.865625 239.257255 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 36.465625 420.039864 \nL 1040.865625 420.039864 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 36.465625 239.257255 \nL 1040.865625 239.257255 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_35\">\n    <!-- Sample 68 Class:['Revelio'] -->\n    <g transform=\"translate(457.09375 233.257255) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-52\" d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-53\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"124.755859\"/>\n     <use xlink:href=\"#DejaVuSans-70\" x=\"222.167969\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"285.644531\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"313.427734\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"374.951172\"/>\n     <use xlink:href=\"#DejaVuSans-36\" x=\"406.738281\"/>\n     <use xlink:href=\"#DejaVuSans-38\" x=\"470.361328\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"533.984375\"/>\n     <use xlink:href=\"#DejaVuSans-43\" x=\"565.771484\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"635.595703\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"663.378906\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"724.658203\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"776.757812\"/>\n     <use xlink:href=\"#DejaVuSans-3a\" x=\"828.857422\"/>\n     <use xlink:href=\"#DejaVuSans-5b\" x=\"862.548828\"/>\n     <use xlink:href=\"#DejaVuSans-27\" x=\"901.5625\"/>\n     <use xlink:href=\"#DejaVuSans-52\" x=\"929.052734\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"994.035156\"/>\n     <use xlink:href=\"#DejaVuSans-76\" x=\"1055.558594\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1114.738281\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"1176.261719\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"1204.044922\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1231.828125\"/>\n     <use xlink:href=\"#DejaVuSans-27\" x=\"1293.009766\"/>\n     <use xlink:href=\"#DejaVuSans-5d\" x=\"1320.5\"/>\n    </g>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_13\">\n     <path d=\"M 988.667188 335.326005 \nL 1033.865625 335.326005 \nQ 1035.865625 335.326005 1035.865625 333.326005 \nL 1035.865625 246.257255 \nQ 1035.865625 244.257255 1033.865625 244.257255 \nL 988.667188 244.257255 \nQ 986.667188 244.257255 986.667188 246.257255 \nL 986.667188 333.326005 \nQ 986.667188 335.326005 988.667188 335.326005 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_46\">\n     <path d=\"M 990.667188 252.355693 \nL 1000.667188 252.355693 \nL 1010.667188 252.355693 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_36\">\n     <!-- aX -->\n     <g transform=\"translate(1018.667188 255.855693) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-58\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_47\">\n     <path d=\"M 990.667188 267.033818 \nL 1000.667188 267.033818 \nL 1010.667188 267.033818 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_37\">\n     <!-- aY -->\n     <g transform=\"translate(1018.667188 270.533818) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-59\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_48\">\n     <path d=\"M 990.667188 281.711943 \nL 1000.667188 281.711943 \nL 1010.667188 281.711943 \n\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_38\">\n     <!-- aZ -->\n     <g transform=\"translate(1018.667188 285.211943) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-5a\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_49\">\n     <path d=\"M 990.667188 296.390068 \nL 1000.667188 296.390068 \nL 1010.667188 296.390068 \n\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_39\">\n     <!-- gX -->\n     <g transform=\"translate(1018.667188 299.890068) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-58\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_50\">\n     <path d=\"M 990.667188 311.068193 \nL 1000.667188 311.068193 \nL 1010.667188 311.068193 \n\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_40\">\n     <!-- gY -->\n     <g transform=\"translate(1018.667188 314.568193) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-59\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_51\">\n     <path d=\"M 990.667188 325.746318 \nL 1000.667188 325.746318 \nL 1010.667188 325.746318 \n\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_41\">\n     <!-- gZ -->\n     <g transform=\"translate(1018.667188 329.246318) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-5a\" x=\"63.476562\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_14\">\n    <path d=\"M 36.465625 636.978995 \nL 1040.865625 636.978995 \nL 1040.865625 456.196386 \nL 36.465625 456.196386 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_15\">\n     <g id=\"line2d_52\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"82.12017\" y=\"636.978995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_42\">\n      <!-- 0 -->\n      <g transform=\"translate(78.93892 651.577432) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_53\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"236.881341\" y=\"636.978995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_43\">\n      <!-- 20 -->\n      <g transform=\"translate(230.518841 651.577432) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_54\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"391.642513\" y=\"636.978995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_44\">\n      <!-- 40 -->\n      <g transform=\"translate(385.280013 651.577432) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_55\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"546.403684\" y=\"636.978995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_45\">\n      <!-- 60 -->\n      <g transform=\"translate(540.041184 651.577432) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_56\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"701.164855\" y=\"636.978995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_46\">\n      <!-- 80 -->\n      <g transform=\"translate(694.802355 651.577432) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_57\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"855.926026\" y=\"636.978995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_47\">\n      <!-- 100 -->\n      <g transform=\"translate(846.382276 651.577432) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_21\">\n     <g id=\"line2d_58\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"1010.687197\" y=\"636.978995\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_48\">\n      <!-- 120 -->\n      <g transform=\"translate(1001.143447 651.577432) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_14\">\n     <g id=\"line2d_59\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"602.830159\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_49\">\n      <!-- 0.2 -->\n      <g transform=\"translate(13.5625 606.629378) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_60\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"568.226064\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_50\">\n      <!-- 0.4 -->\n      <g transform=\"translate(13.5625 572.025282) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_61\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"533.621968\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_51\">\n      <!-- 0.6 -->\n      <g transform=\"translate(13.5625 537.421187) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_62\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"499.017873\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_52\">\n      <!-- 0.8 -->\n      <g transform=\"translate(13.5625 502.817091) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_63\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"464.413777\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_53\">\n      <!-- 1.0 -->\n      <g transform=\"translate(13.5625 468.212996) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_64\">\n    <path d=\"M 82.12017 546.447116 \nL 89.858229 547.57174 \nL 97.596288 549.734504 \nL 105.334346 552.416318 \nL 113.072405 555.768589 \nL 120.810463 559.423644 \nL 128.548522 562.884056 \nL 136.28658 566.020052 \nL 144.024639 569.091164 \nL 151.762697 571.470199 \nL 159.500756 572.551576 \nL 167.238815 571.643217 \nL 174.976873 570.150915 \nL 182.714932 567.317704 \nL 190.45299 563.359864 \nL 198.191049 559.358766 \nL 205.929107 554.838604 \nL 213.667166 549.64799 \nL 221.405224 544.954809 \nL 229.143283 540.391395 \nL 236.881341 535.784728 \nL 244.6194 531.783621 \nL 252.357459 528.863905 \nL 260.095517 526.701152 \nL 267.833576 525.468377 \nL 275.571634 524.278865 \nL 283.309693 523.370501 \nL 291.047751 523.219109 \nL 298.78581 523.435389 \nL 306.523868 524.105847 \nL 314.261927 525.446751 \nL 321.999986 527.696019 \nL 329.738044 530.09668 \nL 337.476103 533.059647 \nL 345.214161 536.455175 \nL 352.95222 539.720937 \nL 360.690278 543.029962 \nL 368.428337 545.646892 \nL 376.166395 547.506862 \nL 383.904454 548.739636 \nL 391.642513 549.86427 \nL 399.380571 550.815876 \nL 407.11863 551.789117 \nL 414.856688 552.805611 \nL 422.594747 553.82211 \nL 430.332805 554.708838 \nL 438.070864 555.48743 \nL 445.808922 556.049747 \nL 453.546981 556.266023 \nL 461.285039 555.963238 \nL 469.023098 555.206272 \nL 476.761157 554.384427 \nL 484.499215 553.259793 \nL 492.237274 552.005393 \nL 499.975332 550.815876 \nL 507.713391 549.907522 \nL 515.451449 549.215439 \nL 523.189508 548.674748 \nL 530.927566 547.917786 \nL 538.665625 548.025927 \nL 546.403684 547.982664 \nL 554.141742 548.263823 \nL 561.879801 548.739636 \nL 569.617859 549.172187 \nL 577.355918 549.064046 \nL 585.093976 549.518223 \nL 592.832035 550.210307 \nL 600.570093 551.875627 \nL 608.308152 553.908619 \nL 616.046211 556.849966 \nL 623.784269 559.921078 \nL 631.522328 562.646154 \nL 639.260386 564.938675 \nL 646.998445 567.101428 \nL 654.736503 568.59373 \nL 662.474562 569.523715 \nL 670.21262 570.259055 \nL 677.950679 569.134421 \nL 685.688737 567.858395 \nL 693.426796 567.880021 \nL 701.164855 569.73999 \nL 708.902913 571.556708 \nL 716.640972 573.200402 \nL 724.37903 572.010884 \nL 732.117089 567.490727 \nL 739.855147 560.461769 \nL 747.593206 553.82211 \nL 755.331264 547.204077 \nL 763.069323 538.899088 \nL 770.807382 533.968011 \nL 778.54544 530.918519 \nL 786.283499 530.183184 \nL 794.021557 529.101812 \nL 801.759616 529.4911 \nL 809.497674 530.507605 \nL 817.235733 532.302686 \nL 824.973791 534.530328 \nL 832.71185 535.871232 \nL 840.449909 536.75796 \nL 848.187967 537.471669 \nL 855.926026 537.817716 \nL 863.664084 538.553051 \nL 871.402143 539.26676 \nL 879.140201 539.893955 \nL 886.87826 540.348143 \nL 894.616318 540.694179 \nL 902.354377 541.083478 \nL 910.092435 540.888823 \nL 917.830494 540.30488 \nL 925.568553 539.742563 \nL 933.306611 539.461405 \nL 941.04467 539.850703 \nL 948.782728 540.066984 \nL 956.520787 539.893955 \nL 964.258845 539.656059 \nL 971.996904 539.288386 \nL 979.734962 538.704443 \nL 987.473021 538.185378 \nL 995.21108 537.752828 \n\" clip-path=\"url(#pe778454874)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path d=\"M 82.12017 594.979354 \nL 89.858229 605.9229 \nL 97.596288 616.542031 \nL 105.334346 625.193056 \nL 113.072405 628.761603 \nL 120.810463 626.317689 \nL 128.548522 621.42986 \nL 136.28658 616.650169 \nL 144.024639 610.334922 \nL 151.762697 600.169968 \nL 159.500756 587.73412 \nL 167.238815 576.574304 \nL 174.976873 567.036545 \nL 182.714932 557.066241 \nL 190.45299 546.187583 \nL 198.191049 535.265663 \nL 205.929107 523.803052 \nL 213.667166 514.135532 \nL 221.405224 504.662667 \nL 229.143283 496.530706 \nL 236.881341 492.226814 \nL 244.6194 489.134081 \nL 252.357459 487.728288 \nL 260.095517 488.204091 \nL 267.833576 488.420371 \nL 275.571634 487.836428 \nL 283.309693 485.889945 \nL 291.047751 482.278146 \nL 298.78581 478.450067 \nL 306.523868 474.513848 \nL 314.261927 470.729021 \nL 321.999986 470.382984 \nL 329.738044 473.778512 \nL 337.476103 480.850727 \nL 345.214161 489.977556 \nL 352.95222 499.796468 \nL 360.690278 509.463988 \nL 368.428337 519.737077 \nL 376.166395 528.647625 \nL 383.904454 533.794993 \nL 391.642513 536.303783 \nL 399.380571 537.579809 \nL 407.11863 539.7642 \nL 414.856688 542.48927 \nL 422.594747 545.863162 \nL 430.332805 550.599605 \nL 438.070864 556.417415 \nL 445.808922 562.905682 \nL 453.546981 570.821372 \nL 461.285039 579.429141 \nL 469.023098 587.712494 \nL 476.761157 594.460294 \nL 484.499215 599.542771 \nL 492.237274 602.743649 \nL 499.975332 601.705527 \nL 507.713391 597.985586 \nL 515.451449 595.844458 \nL 523.189508 594.698198 \nL 530.927566 591.908241 \nL 538.665625 587.344827 \nL 546.403684 584.533241 \nL 554.141742 585.355091 \nL 561.879801 588.491087 \nL 569.617859 591.99475 \nL 577.355918 596.62305 \nL 585.093976 603.154572 \nL 592.832035 608.604719 \nL 600.570093 612.692325 \nL 608.308152 614.206255 \nL 616.046211 612.605816 \nL 623.784269 607.631477 \nL 631.522328 601.554132 \nL 639.260386 593.595192 \nL 646.998445 586.78251 \nL 654.736503 580.964695 \nL 662.474562 575.990356 \nL 670.21262 573.373425 \nL 677.950679 575.384787 \nL 685.688737 581.37562 \nL 693.426796 586.89065 \nL 701.164855 586.760884 \nL 708.902913 576.985224 \nL 716.640972 558.731566 \nL 724.37903 534.076151 \nL 732.117089 510.134435 \nL 739.855147 487.165971 \nL 747.593206 468.631155 \nL 755.331264 464.413777 \nL 763.069323 464.413777 \nL 770.807382 464.413777 \nL 778.54544 474.903147 \nL 786.283499 489.588257 \nL 794.021557 505.419628 \nL 801.759616 516.492941 \nL 809.497674 525.857676 \nL 817.235733 531.56735 \nL 824.973791 535.395429 \nL 832.71185 539.893955 \nL 840.449909 544.068081 \nL 848.187967 545.322482 \nL 855.926026 543.657157 \nL 863.664084 542.402756 \nL 871.402143 542.467644 \nL 879.140201 543.289484 \nL 886.87826 544.67365 \nL 894.616318 548.93428 \nL 902.354377 552.740733 \nL 910.092435 557.022989 \nL 917.830494 560.721302 \nL 925.568553 563.705905 \nL 933.306611 565.219833 \nL 941.04467 565.198207 \nL 948.782728 565.479366 \nL 956.520787 564.549381 \nL 964.258845 561.845936 \nL 971.996904 558.125992 \nL 979.734962 554.687212 \nL 987.473021 551.962141 \nL 995.21108 550.750998 \n\" clip-path=\"url(#pe778454874)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path d=\"M 82.12017 576.790579 \nL 89.858229 578.801941 \nL 97.596288 580.899812 \nL 105.334346 581.094461 \nL 113.072405 576.401281 \nL 120.810463 569.069538 \nL 128.548522 562.105463 \nL 136.28658 556.763457 \nL 144.024639 550.188681 \nL 151.762697 540.30488 \nL 159.500756 530.118306 \nL 167.238815 519.17476 \nL 174.976873 512.362077 \nL 182.714932 505.765675 \nL 190.45299 499.450421 \nL 198.191049 493.632607 \nL 205.929107 489.523369 \nL 213.667166 488.463623 \nL 221.405224 490.539873 \nL 229.143283 496.703724 \nL 236.881341 507.387738 \nL 244.6194 518.893601 \nL 252.357459 527.609505 \nL 260.095517 530.550857 \nL 267.833576 530.42109 \nL 275.571634 529.188316 \nL 283.309693 526.246975 \nL 291.047751 523.024465 \nL 298.78581 522.418896 \nL 306.523868 522.029597 \nL 314.261927 522.462148 \nL 321.999986 525.295359 \nL 329.738044 530.658997 \nL 337.476103 536.04425 \nL 345.214161 538.812584 \nL 352.95222 537.774464 \nL 360.690278 535.244037 \nL 368.428337 532.540592 \nL 376.166395 527.782523 \nL 383.904454 523.478641 \nL 391.642513 519.585684 \nL 399.380571 517.898733 \nL 407.11863 518.35291 \nL 414.856688 519.801955 \nL 422.594747 522.873072 \nL 430.332805 526.593011 \nL 438.070864 529.555989 \nL 445.808922 531.978275 \nL 453.546981 532.778488 \nL 461.285039 532.778488 \nL 469.023098 532.345938 \nL 476.761157 531.502472 \nL 484.499215 531.524098 \nL 492.237274 531.394332 \nL 499.975332 530.31295 \nL 507.713391 529.27483 \nL 515.451449 530.594109 \nL 523.189508 534.811486 \nL 530.927566 540.30488 \nL 538.665625 546.447116 \nL 546.403684 552.546084 \nL 554.141742 558.558543 \nL 561.879801 563.035448 \nL 569.617859 567.858395 \nL 577.355918 571.729726 \nL 585.093976 574.62782 \nL 592.832035 576.206631 \nL 600.570093 574.908979 \nL 608.308152 571.037648 \nL 616.046211 565.565875 \nL 623.784269 560.548278 \nL 631.522328 555.530687 \nL 639.260386 549.950774 \nL 646.998445 543.786923 \nL 654.736503 537.471669 \nL 662.474562 531.35107 \nL 670.21262 528.106944 \nL 677.950679 529.664129 \nL 685.688737 533.924759 \nL 693.426796 535.914494 \nL 701.164855 529.599241 \nL 708.902913 514.395065 \nL 716.640972 489.869416 \nL 724.37903 464.67331 \nL 732.117089 464.413777 \nL 739.855147 464.413777 \nL 747.593206 464.413777 \nL 755.331264 464.413777 \nL 763.069323 464.413777 \nL 770.807382 488.874548 \nL 778.54544 515.433195 \nL 786.283499 539.439779 \nL 794.021557 550.426587 \nL 801.759616 554.233035 \nL 809.497674 548.415215 \nL 817.235733 534.854738 \nL 824.973791 520.991477 \nL 832.71185 511.864638 \nL 840.449909 505.765675 \nL 848.187967 505.787301 \nL 855.926026 509.117941 \nL 863.664084 514.697849 \nL 871.402143 520.731945 \nL 879.140201 527.977178 \nL 886.87826 533.254302 \nL 894.616318 540.564413 \nL 902.354377 547.160825 \nL 910.092435 552.481201 \nL 917.830494 557.001358 \nL 925.568553 558.796449 \nL 933.306611 556.698573 \nL 941.04467 549.583112 \nL 948.782728 541.840439 \nL 956.520787 536.476801 \nL 964.258845 534.638468 \nL 971.996904 535.308915 \nL 979.734962 537.060755 \nL 987.473021 538.444911 \nL 995.21108 539.028854 \n\" clip-path=\"url(#pe778454874)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path d=\"M 82.12017 551.23026 \nL 89.858229 550.625686 \nL 97.596288 550.139932 \nL 105.334346 549.456109 \nL 113.072405 548.761778 \nL 120.810463 548.212651 \nL 128.548522 547.972413 \nL 136.28658 547.911681 \nL 144.024639 547.814019 \nL 151.762697 547.634462 \nL 159.500756 547.344053 \nL 167.238815 547.132877 \nL 174.976873 547.090615 \nL 182.714932 546.842458 \nL 190.45299 546.546769 \nL 198.191049 546.443785 \nL 205.929107 546.493957 \nL 213.667166 546.747384 \nL 221.405224 547.21999 \nL 229.143283 547.848319 \nL 236.881341 548.339395 \nL 244.6194 548.666745 \nL 252.357459 548.901712 \nL 260.095517 549.342606 \nL 267.833576 549.712228 \nL 275.571634 550.042218 \nL 283.309693 550.496312 \nL 291.047751 551.05339 \nL 298.78581 551.55498 \nL 306.523868 552.032861 \nL 314.261927 552.526531 \nL 321.999986 552.946325 \nL 329.738044 553.278954 \nL 337.476103 553.437354 \nL 345.214161 553.537708 \nL 352.95222 553.727768 \nL 360.690278 553.975967 \nL 368.428337 554.377244 \nL 376.166395 554.910536 \nL 383.904454 555.409527 \nL 391.642513 555.831915 \nL 399.380571 556.204166 \nL 407.11863 556.46028 \nL 414.856688 556.550038 \nL 422.594747 556.354651 \nL 430.332805 555.998276 \nL 438.070864 555.779144 \nL 445.808922 555.628659 \nL 453.546981 555.517755 \nL 461.285039 555.266957 \nL 469.023098 554.907901 \nL 476.761157 554.517174 \nL 484.499215 554.126452 \nL 492.237274 553.833398 \nL 499.975332 553.535068 \nL 507.713391 553.263124 \nL 515.451449 553.152261 \nL 523.189508 553.062462 \nL 530.927566 552.909338 \nL 538.665625 552.708722 \nL 546.403684 552.521256 \nL 554.141742 552.270463 \nL 561.879801 551.916677 \nL 569.617859 551.541785 \nL 577.355918 551.135233 \nL 585.093976 550.623046 \nL 592.832035 549.659406 \nL 600.570093 548.701045 \nL 608.308152 548.067446 \nL 616.046211 547.615992 \nL 623.784269 547.24638 \nL 631.522328 546.937491 \nL 639.260386 546.549409 \nL 646.998445 546.229928 \nL 654.736503 545.95803 \nL 662.474562 545.64914 \nL 670.21262 545.321739 \nL 677.950679 544.822794 \nL 685.688737 544.099391 \nL 693.426796 543.191213 \nL 701.164855 542.586634 \nL 708.902913 542.375417 \nL 716.640972 542.507431 \nL 724.37903 543.43145 \nL 732.117089 544.68023 \nL 739.855147 545.6465 \nL 747.593206 546.108515 \nL 755.331264 546.253719 \nL 763.069323 548.466088 \nL 770.807382 549.910245 \nL 778.54544 549.970936 \nL 786.283499 550.071279 \nL 794.021557 550.227044 \nL 801.759616 549.936635 \nL 809.497674 550.09767 \nL 817.235733 550.295697 \nL 824.973791 550.752421 \nL 832.71185 551.238175 \nL 840.449909 551.951024 \nL 848.187967 552.479035 \nL 855.926026 552.774729 \nL 863.664084 552.917299 \nL 871.402143 553.012332 \nL 879.140201 552.909338 \nL 886.87826 552.911978 \nL 894.616318 553.043992 \nL 902.354377 552.943684 \nL 910.092435 552.888232 \nL 917.830494 552.692892 \nL 925.568553 552.257268 \nL 933.306611 551.718701 \nL 941.04467 551.415092 \nL 948.782728 551.433562 \nL 956.520787 551.454667 \nL 964.258845 551.478418 \nL 971.996904 551.496929 \nL 979.734962 551.496929 \nL 987.473021 551.465223 \nL 995.21108 551.314738 \n\" clip-path=\"url(#pe778454874)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_68\">\n    <path d=\"M 82.12017 552.428863 \nL 89.858229 553.234099 \nL 97.596288 554.171308 \nL 105.334346 555.105923 \nL 113.072405 556.193616 \nL 120.810463 557.154612 \nL 128.548522 557.87801 \nL 136.28658 558.384875 \nL 144.024639 558.844244 \nL 151.762697 559.308935 \nL 159.500756 559.697017 \nL 167.238815 559.908234 \nL 174.976873 559.327405 \nL 182.714932 558.928763 \nL 190.45299 558.411301 \nL 198.191049 557.814643 \nL 205.929107 557.067499 \nL 213.667166 556.130244 \nL 221.405224 554.995013 \nL 229.143283 553.883533 \nL 236.881341 552.882952 \nL 244.6194 552.196536 \nL 252.357459 551.787302 \nL 260.095517 551.903487 \nL 267.833576 552.072442 \nL 275.571634 552.067161 \nL 283.309693 551.990599 \nL 291.047751 551.795259 \nL 298.78581 551.452032 \nL 306.523868 551.116763 \nL 314.261927 550.784081 \nL 321.999986 550.409189 \nL 329.738044 550.11086 \nL 337.476103 549.999997 \nL 345.214161 550.073919 \nL 352.95222 550.295697 \nL 360.690278 550.501582 \nL 368.428337 550.588746 \nL 376.166395 550.646797 \nL 383.904454 550.588746 \nL 391.642513 550.42503 \nL 399.380571 550.153122 \nL 407.11863 549.812531 \nL 414.856688 549.537993 \nL 422.594747 549.342606 \nL 430.332805 549.226422 \nL 438.070864 549.213232 \nL 445.808922 549.295074 \nL 453.546981 549.440279 \nL 461.285039 549.596044 \nL 469.023098 549.743888 \nL 476.761157 549.823091 \nL 484.499215 549.820451 \nL 492.237274 549.77032 \nL 499.975332 549.701668 \nL 507.713391 549.585483 \nL 515.451449 549.374267 \nL 523.189508 549.120839 \nL 530.927566 548.970354 \nL 538.665625 548.933373 \nL 546.403684 549.068027 \nL 554.141742 549.326776 \nL 561.879801 549.77032 \nL 569.617859 550.279815 \nL 577.355918 550.876474 \nL 585.093976 551.597237 \nL 592.832035 552.486955 \nL 600.570093 553.601075 \nL 608.308152 554.620122 \nL 616.046211 555.528305 \nL 623.784269 556.262258 \nL 631.522328 556.885308 \nL 639.260386 557.463496 \nL 646.998445 557.983598 \nL 654.736503 558.39279 \nL 662.474562 558.712271 \nL 670.21262 558.889141 \nL 677.950679 558.947233 \nL 685.688737 559.095083 \nL 693.426796 559.559727 \nL 701.164855 560.291041 \nL 708.902913 561.143772 \nL 716.640972 562.123243 \nL 724.37903 562.18397 \nL 732.117089 560.399268 \nL 739.855147 558.596092 \nL 747.593206 556.080113 \nL 755.331264 553.936345 \nL 763.069323 551.119397 \nL 770.807382 548.84889 \nL 778.54544 547.890581 \nL 786.283499 546.837188 \nL 794.021557 547.56846 \nL 801.759616 549.15514 \nL 809.497674 550.060729 \nL 817.235733 550.892355 \nL 824.973791 551.140508 \nL 832.71185 550.910826 \nL 840.449909 550.398639 \nL 848.187967 549.548543 \nL 855.926026 548.716927 \nL 863.664084 548.085916 \nL 871.402143 547.637102 \nL 879.140201 547.354603 \nL 886.87826 547.24374 \nL 894.616318 547.20944 \nL 902.354377 547.27276 \nL 910.092435 547.634462 \nL 917.830494 548.104387 \nL 925.568553 548.740667 \nL 933.306611 549.498371 \nL 941.04467 550.195343 \nL 948.782728 550.612496 \nL 956.520787 550.739231 \nL 964.258845 550.659987 \nL 971.996904 550.546484 \nL 979.734962 550.477842 \nL 987.473021 550.498952 \nL 995.21108 550.594026 \n\" clip-path=\"url(#pe778454874)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_69\">\n    <path d=\"M 82.12017 553.352877 \nL 89.858229 552.611008 \nL 97.596288 551.700185 \nL 105.334346 550.546484 \nL 113.072405 549.136669 \nL 120.810463 547.54735 \nL 128.548522 546.090045 \nL 136.28658 544.788453 \nL 144.024639 543.592485 \nL 151.762697 542.457259 \nL 159.500756 541.451408 \nL 167.238815 540.733285 \nL 174.976873 540.337283 \nL 182.714932 540.044224 \nL 190.45299 539.880549 \nL 198.191049 539.906939 \nL 205.929107 540.107596 \nL 213.667166 540.495678 \nL 221.405224 541.097617 \nL 229.143283 541.889662 \nL 236.881341 542.795211 \nL 244.6194 543.764121 \nL 252.357459 544.743592 \nL 260.095517 545.794345 \nL 267.833576 546.744754 \nL 275.571634 547.608082 \nL 283.309693 548.426508 \nL 291.047751 549.231702 \nL 298.78581 550.08184 \nL 306.523868 551.011133 \nL 314.261927 552.038136 \nL 321.999986 553.265759 \nL 329.738044 554.601651 \nL 337.476103 555.884732 \nL 345.214161 557.046389 \nL 352.95222 558.070715 \nL 360.690278 558.841604 \nL 368.428337 559.432988 \nL 376.166395 559.818435 \nL 383.904454 559.963644 \nL 391.642513 560.011181 \nL 399.380571 560.069274 \nL 407.11863 560.195967 \nL 414.856688 560.354413 \nL 422.594747 560.515448 \nL 430.332805 560.636866 \nL 438.070864 560.73722 \nL 445.808922 560.739855 \nL 453.546981 560.660657 \nL 461.285039 560.43625 \nL 469.023098 560.024372 \nL 476.761157 559.409243 \nL 484.499215 558.635714 \nL 492.237274 557.780296 \nL 499.975332 556.848367 \nL 507.713391 555.937544 \nL 515.451449 555.166609 \nL 523.189508 554.488107 \nL 530.927566 553.820207 \nL 538.665625 553.207672 \nL 546.403684 552.737748 \nL 554.141742 552.389283 \nL 561.879801 552.075077 \nL 569.617859 551.652689 \nL 577.355918 551.188045 \nL 585.093976 550.638877 \nL 592.832035 549.846872 \nL 600.570093 548.85681 \nL 608.308152 547.753287 \nL 616.046211 546.604819 \nL 623.784269 545.480185 \nL 631.522328 544.469013 \nL 639.260386 543.566105 \nL 646.998445 542.824231 \nL 654.736503 542.264554 \nL 662.474562 541.83949 \nL 670.21262 541.535881 \nL 677.950679 541.306194 \nL 685.688737 541.002584 \nL 693.426796 540.516788 \nL 701.164855 539.867359 \nL 708.902913 539.056843 \nL 716.640972 538.172405 \nL 724.37903 537.633838 \nL 732.117089 537.776403 \nL 739.855147 538.31234 \nL 747.593206 539.413264 \nL 755.331264 540.918112 \nL 763.069323 543.199123 \nL 770.807382 546.010842 \nL 778.54544 548.151929 \nL 786.283499 550.021107 \nL 794.021557 551.349085 \nL 801.759616 551.987964 \nL 809.497674 552.4764 \nL 817.235733 552.684931 \nL 824.973791 552.819585 \nL 832.71185 552.875037 \nL 840.449909 552.882952 \nL 848.187967 552.77737 \nL 855.926026 552.642715 \nL 863.664084 552.61369 \nL 871.402143 552.711362 \nL 879.140201 552.819585 \nL 886.87826 552.959515 \nL 894.616318 553.139025 \nL 902.354377 553.218269 \nL 910.092435 553.199752 \nL 917.830494 553.096809 \nL 925.568553 552.882952 \nL 933.306611 552.584623 \nL 941.04467 552.246713 \nL 948.782728 551.951024 \nL 956.520787 551.631537 \nL 964.258845 551.317378 \nL 971.996904 551.092971 \nL 979.734962 550.953041 \nL 987.473021 550.889715 \nL 995.21108 550.908186 \n\" clip-path=\"url(#pe778454874)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 36.465625 636.978995 \nL 36.465625 456.196386 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 1040.865625 636.978995 \nL 1040.865625 456.196386 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path d=\"M 36.465625 636.978995 \nL 1040.865625 636.978995 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 36.465625 456.196386 \nL 1040.865625 456.196386 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_54\">\n    <!-- Sample 51 Class:['AvadaKedavra'] -->\n    <g transform=\"translate(435.959688 450.196386) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-4b\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 2694 \nL 3353 4666 \nL 4166 4666 \nL 1850 2491 \nL 4331 0 \nL 3500 0 \nL 1259 2247 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-53\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"124.755859\"/>\n     <use xlink:href=\"#DejaVuSans-70\" x=\"222.167969\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"285.644531\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"313.427734\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"374.951172\"/>\n     <use xlink:href=\"#DejaVuSans-35\" x=\"406.738281\"/>\n     <use xlink:href=\"#DejaVuSans-31\" x=\"470.361328\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"533.984375\"/>\n     <use xlink:href=\"#DejaVuSans-43\" x=\"565.771484\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"635.595703\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"663.378906\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"724.658203\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"776.757812\"/>\n     <use xlink:href=\"#DejaVuSans-3a\" x=\"828.857422\"/>\n     <use xlink:href=\"#DejaVuSans-5b\" x=\"862.548828\"/>\n     <use xlink:href=\"#DejaVuSans-27\" x=\"901.5625\"/>\n     <use xlink:href=\"#DejaVuSans-41\" x=\"929.052734\"/>\n     <use xlink:href=\"#DejaVuSans-76\" x=\"991.585938\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"1050.765625\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"1112.044922\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"1175.521484\"/>\n     <use xlink:href=\"#DejaVuSans-4b\" x=\"1236.800781\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"1297.376953\"/>\n     <use xlink:href=\"#DejaVuSans-64\" x=\"1358.900391\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"1422.376953\"/>\n     <use xlink:href=\"#DejaVuSans-76\" x=\"1483.65625\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"1542.835938\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"1583.949219\"/>\n     <use xlink:href=\"#DejaVuSans-27\" x=\"1645.228516\"/>\n     <use xlink:href=\"#DejaVuSans-5d\" x=\"1672.71875\"/>\n    </g>\n   </g>\n   <g id=\"legend_3\">\n    <g id=\"patch_19\">\n     <path d=\"M 43.465625 552.265136 \nL 88.664063 552.265136 \nQ 90.664063 552.265136 90.664063 550.265136 \nL 90.664063 463.196386 \nQ 90.664063 461.196386 88.664063 461.196386 \nL 43.465625 461.196386 \nQ 41.465625 461.196386 41.465625 463.196386 \nL 41.465625 550.265136 \nQ 41.465625 552.265136 43.465625 552.265136 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_70\">\n     <path d=\"M 45.465625 469.294823 \nL 55.465625 469.294823 \nL 65.465625 469.294823 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_55\">\n     <!-- aX -->\n     <g transform=\"translate(73.465625 472.794823) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-58\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_71\">\n     <path d=\"M 45.465625 483.972948 \nL 55.465625 483.972948 \nL 65.465625 483.972948 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_56\">\n     <!-- aY -->\n     <g transform=\"translate(73.465625 487.472948) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-59\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_72\">\n     <path d=\"M 45.465625 498.651073 \nL 55.465625 498.651073 \nL 65.465625 498.651073 \n\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_57\">\n     <!-- aZ -->\n     <g transform=\"translate(73.465625 502.151073) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-5a\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_73\">\n     <path d=\"M 45.465625 513.329198 \nL 55.465625 513.329198 \nL 65.465625 513.329198 \n\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_58\">\n     <!-- gX -->\n     <g transform=\"translate(73.465625 516.829198) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-58\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_74\">\n     <path d=\"M 45.465625 528.007323 \nL 55.465625 528.007323 \nL 65.465625 528.007323 \n\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_59\">\n     <!-- gY -->\n     <g transform=\"translate(73.465625 531.507323) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-59\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_75\">\n     <path d=\"M 45.465625 542.685448 \nL 55.465625 542.685448 \nL 65.465625 542.685448 \n\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_60\">\n     <!-- gZ -->\n     <g transform=\"translate(73.465625 546.185448) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-5a\" x=\"63.476562\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_20\">\n    <path d=\"M 36.465625 853.918125 \nL 1040.865625 853.918125 \nL 1040.865625 673.135516 \nL 36.465625 673.135516 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_7\">\n    <g id=\"xtick_22\">\n     <g id=\"line2d_76\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"82.12017\" y=\"853.918125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_61\">\n      <!-- 0 -->\n      <g transform=\"translate(78.93892 868.516562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_23\">\n     <g id=\"line2d_77\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"236.881341\" y=\"853.918125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_62\">\n      <!-- 20 -->\n      <g transform=\"translate(230.518841 868.516562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_24\">\n     <g id=\"line2d_78\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"391.642513\" y=\"853.918125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_63\">\n      <!-- 40 -->\n      <g transform=\"translate(385.280013 868.516562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_25\">\n     <g id=\"line2d_79\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"546.403684\" y=\"853.918125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_64\">\n      <!-- 60 -->\n      <g transform=\"translate(540.041184 868.516562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_26\">\n     <g id=\"line2d_80\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"701.164855\" y=\"853.918125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_65\">\n      <!-- 80 -->\n      <g transform=\"translate(694.802355 868.516562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_27\">\n     <g id=\"line2d_81\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"855.926026\" y=\"853.918125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_66\">\n      <!-- 100 -->\n      <g transform=\"translate(846.382276 868.516562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_28\">\n     <g id=\"line2d_82\">\n      <g>\n       <use xlink:href=\"#m0a4955e443\" x=\"1010.687197\" y=\"853.918125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_67\">\n      <!-- 120 -->\n      <g transform=\"translate(1001.143447 868.516562) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_8\">\n    <g id=\"ytick_19\">\n     <g id=\"line2d_83\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"824.867907\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_68\">\n      <!-- 0.45 -->\n      <g transform=\"translate(7.2 828.667126) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_84\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"792.940206\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_69\">\n      <!-- 0.50 -->\n      <g transform=\"translate(7.2 796.739425) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_85\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"761.012504\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_70\">\n      <!-- 0.55 -->\n      <g transform=\"translate(7.2 764.811723) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_86\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"729.084803\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_71\">\n      <!-- 0.60 -->\n      <g transform=\"translate(7.2 732.884021) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_87\">\n      <g>\n       <use xlink:href=\"#mc47b2e7a22\" x=\"36.465625\" y=\"697.157101\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_72\">\n      <!-- 0.65 -->\n      <g transform=\"translate(7.2 700.95632) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_88\">\n    <path d=\"M 82.12017 743.851383 \nL 89.858229 743.691718 \nL 97.596288 743.771531 \nL 105.334346 744.729369 \nL 113.072405 745.128474 \nL 120.810463 745.367915 \nL 128.548522 745.447766 \nL 136.28658 745.607393 \nL 144.024639 745.208288 \nL 151.762697 745.52758 \nL 159.500756 745.447766 \nL 167.238815 745.687207 \nL 174.976873 745.687207 \nL 182.714932 745.926647 \nL 190.45299 746.166126 \nL 198.191049 745.926647 \nL 205.929107 745.52758 \nL 213.667166 746.006499 \nL 221.405224 746.006499 \nL 229.143283 746.884485 \nL 236.881341 748.00195 \nL 244.6194 749.199266 \nL 252.357459 750.316731 \nL 260.095517 751.274568 \nL 267.833576 752.072741 \nL 275.571634 752.711287 \nL 283.309693 753.190206 \nL 291.047751 754.547149 \nL 298.78581 755.5848 \nL 306.523868 757.500476 \nL 314.261927 759.495926 \nL 321.999986 761.411602 \nL 329.738044 763.087799 \nL 337.476103 764.843809 \nL 345.214161 765.881461 \nL 352.95222 767.318217 \nL 360.690278 768.914601 \nL 368.428337 769.952252 \nL 376.166395 771.548636 \nL 383.904454 772.825765 \nL 391.642513 774.262484 \nL 399.380571 774.980881 \nL 407.11863 774.901068 \nL 414.856688 774.821216 \nL 422.594747 774.901068 \nL 430.332805 774.661589 \nL 438.070864 774.18267 \nL 445.808922 773.065206 \nL 453.546981 772.42666 \nL 461.285039 771.149568 \nL 469.023098 770.91009 \nL 476.761157 770.670649 \nL 484.499215 770.032066 \nL 492.237274 769.39352 \nL 499.975332 769.473333 \nL 507.713391 769.39352 \nL 515.451449 768.994414 \nL 523.189508 768.834787 \nL 530.927566 768.435682 \nL 538.665625 767.15859 \nL 546.403684 766.041126 \nL 554.141742 764.684183 \nL 561.879801 763.087799 \nL 569.617859 760.932683 \nL 577.355918 758.857381 \nL 585.093976 756.542638 \nL 592.832035 754.227857 \nL 600.570093 752.31222 \nL 608.308152 750.556171 \nL 616.046211 748.879974 \nL 623.784269 747.762509 \nL 631.522328 745.846834 \nL 639.260386 743.771531 \nL 646.998445 742.49444 \nL 654.736503 741.376975 \nL 662.474562 740.578764 \nL 670.21262 739.780591 \nL 677.950679 739.940218 \nL 685.688737 738.822754 \nL 693.426796 738.184208 \nL 701.164855 737.22637 \nL 708.902913 737.385997 \nL 716.640972 736.747451 \nL 724.37903 737.465811 \nL 732.117089 737.465811 \nL 739.855147 736.827265 \nL 747.593206 735.7098 \nL 755.331264 736.348346 \nL 763.069323 736.986892 \nL 770.807382 737.785102 \nL 778.54544 738.503462 \nL 786.283499 739.142046 \nL 794.021557 738.184208 \nL 801.759616 737.944729 \nL 809.497674 737.465811 \nL 817.235733 736.907078 \nL 824.973791 735.310695 \nL 832.71185 734.831776 \nL 840.449909 735.151068 \nL 848.187967 734.592335 \nL 855.926026 734.672149 \nL 863.664084 734.352857 \nL 871.402143 734.113417 \nL 879.140201 734.113417 \nL 886.87826 733.794125 \nL 894.616318 734.113417 \nL 902.354377 734.592335 \nL 910.092435 735.071254 \nL 917.830494 735.550173 \nL 925.568553 736.108905 \nL 933.306611 737.066743 \nL 941.04467 738.503462 \nL 948.782728 740.25951 \nL 956.520787 741.696229 \nL 964.258845 742.654067 \nL 971.996904 743.691718 \nL 979.734962 744.809183 \nL 987.473021 746.006499 \nL 995.21108 746.645045 \n\" clip-path=\"url(#pe5db293d38)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_89\">\n    <path d=\"M 82.12017 737.944729 \nL 89.858229 731.878487 \nL 97.596288 725.732393 \nL 105.334346 721.821267 \nL 113.072405 719.267046 \nL 120.810463 718.548648 \nL 128.548522 716.553197 \nL 136.28658 714.238416 \nL 144.024639 712.003487 \nL 151.762697 713.360392 \nL 159.500756 715.675173 \nL 167.238815 714.238416 \nL 174.976873 711.923674 \nL 182.714932 709.848371 \nL 190.45299 707.134523 \nL 198.191049 703.782091 \nL 205.929107 697.396557 \nL 213.667166 691.410128 \nL 221.405224 686.700791 \nL 229.143283 685.264034 \nL 236.881341 688.217361 \nL 244.6194 694.123976 \nL 252.357459 699.870964 \nL 260.095517 702.74444 \nL 267.833576 703.223359 \nL 275.571634 703.303172 \nL 283.309693 703.303172 \nL 291.047751 702.504999 \nL 298.78581 703.063732 \nL 306.523868 703.622464 \nL 314.261927 707.932696 \nL 321.999986 716.233905 \nL 329.738044 723.497464 \nL 337.476103 726.770045 \nL 345.214161 727.488404 \nL 352.95222 728.526055 \nL 360.690278 732.67666 \nL 368.428337 738.024543 \nL 376.166395 746.006499 \nL 383.904454 757.660103 \nL 391.642513 769.712812 \nL 399.380571 779.370927 \nL 407.11863 786.873925 \nL 414.856688 793.818211 \nL 422.594747 800.283578 \nL 430.332805 804.35435 \nL 438.070864 805.551647 \nL 445.808922 803.87545 \nL 453.546981 802.35888 \nL 461.285039 801.161583 \nL 469.023098 801.001956 \nL 476.761157 802.518507 \nL 484.499215 804.913101 \nL 492.237274 808.345328 \nL 499.975332 813.93267 \nL 507.713391 821.276042 \nL 515.451449 828.220309 \nL 523.189508 836.681165 \nL 530.927566 844.34381 \nL 538.665625 845.700734 \nL 546.403684 843.226345 \nL 554.141742 839.873932 \nL 561.879801 832.929646 \nL 569.617859 821.355856 \nL 577.355918 809.303147 \nL 585.093976 798.687194 \nL 592.832035 789.907066 \nL 600.570093 782.164588 \nL 608.308152 774.342335 \nL 616.046211 769.792625 \nL 623.784269 770.670649 \nL 631.522328 774.581776 \nL 639.260386 777.774543 \nL 646.998445 779.2113 \nL 654.736503 780.169137 \nL 662.474562 784.798623 \nL 670.21262 786.315193 \nL 677.950679 781.047124 \nL 685.688737 774.102857 \nL 693.426796 768.036615 \nL 701.164855 761.251975 \nL 708.902913 761.491415 \nL 716.640972 752.870952 \nL 724.37903 755.026068 \nL 732.117089 762.209775 \nL 739.855147 764.285077 \nL 747.593206 772.6661 \nL 755.331264 784.00045 \nL 763.069323 791.423636 \nL 770.807382 796.611892 \nL 778.54544 797.090811 \nL 786.283499 789.587812 \nL 794.021557 777.375438 \nL 801.759616 772.6661 \nL 809.497674 771.708301 \nL 817.235733 768.196242 \nL 824.973791 759.735405 \nL 832.71185 754.467335 \nL 840.449909 751.514009 \nL 848.187967 751.673636 \nL 855.926026 753.429684 \nL 863.664084 753.748938 \nL 871.402143 751.913114 \nL 879.140201 750.316731 \nL 886.87826 744.170637 \nL 894.616318 737.705289 \nL 902.354377 734.911589 \nL 910.092435 733.315206 \nL 917.830494 733.634498 \nL 925.568553 736.986892 \nL 933.306611 744.649556 \nL 941.04467 753.030579 \nL 948.782728 760.13451 \nL 956.520787 768.036615 \nL 964.258845 775.060695 \nL 971.996904 780.72787 \nL 979.734962 783.122426 \nL 987.473021 780.648056 \nL 995.21108 775.220322 \n\" clip-path=\"url(#pe5db293d38)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_90\">\n    <path d=\"M 82.12017 683.587837 \nL 89.858229 681.991453 \nL 97.596288 682.629999 \nL 105.334346 681.352908 \nL 113.072405 684.066756 \nL 120.810463 689.095347 \nL 128.548522 694.842335 \nL 136.28658 700.669137 \nL 144.024639 706.974858 \nL 151.762697 712.242928 \nL 159.500756 715.036627 \nL 167.238815 719.426673 \nL 174.976873 724.13601 \nL 182.714932 730.20229 \nL 190.45299 739.221859 \nL 198.191049 748.401055 \nL 205.929107 753.509498 \nL 213.667166 754.148043 \nL 221.405224 750.556171 \nL 229.143283 744.968848 \nL 236.881341 740.419137 \nL 244.6194 739.620926 \nL 252.357459 743.212799 \nL 260.095517 751.593822 \nL 267.833576 760.294137 \nL 275.571634 766.919112 \nL 283.309693 771.309195 \nL 291.047751 774.18267 \nL 298.78581 774.741403 \nL 306.523868 774.262484 \nL 314.261927 775.140508 \nL 321.999986 776.018532 \nL 329.738044 776.657078 \nL 337.476103 779.291113 \nL 345.214161 784.160077 \nL 352.95222 789.188706 \nL 360.690278 793.898044 \nL 368.428337 797.170624 \nL 376.166395 799.32574 \nL 383.904454 799.565199 \nL 391.642513 798.767008 \nL 399.380571 798.527548 \nL 407.11863 799.565199 \nL 414.856688 801.001956 \nL 422.594747 803.87545 \nL 430.332805 806.429653 \nL 438.070864 809.462793 \nL 445.808922 811.857368 \nL 453.546981 811.937182 \nL 461.285039 810.42063 \nL 469.023098 806.429653 \nL 476.761157 801.879961 \nL 484.499215 798.607381 \nL 492.237274 795.095322 \nL 499.975332 793.259479 \nL 507.713391 793.179665 \nL 515.451449 793.419125 \nL 523.189508 791.982368 \nL 530.927566 787.432696 \nL 538.665625 779.770032 \nL 546.403684 771.548636 \nL 554.141742 764.285077 \nL 561.879801 758.378462 \nL 569.617859 754.467335 \nL 577.355918 752.870952 \nL 585.093976 749.039601 \nL 592.832035 744.25045 \nL 600.570093 738.024543 \nL 608.308152 732.67666 \nL 616.046211 727.727882 \nL 623.784269 730.282104 \nL 631.522328 737.22637 \nL 639.260386 740.339324 \nL 646.998445 738.024543 \nL 654.736503 735.151068 \nL 662.474562 731.239941 \nL 670.21262 728.206801 \nL 677.950679 726.211312 \nL 685.688737 727.16915 \nL 693.426796 726.849858 \nL 701.164855 724.774556 \nL 708.902913 724.774556 \nL 716.640972 730.840836 \nL 724.37903 734.352857 \nL 732.117089 732.277554 \nL 739.855147 733.95379 \nL 747.593206 733.873976 \nL 755.331264 729.962812 \nL 763.069323 726.370939 \nL 770.807382 726.131499 \nL 778.54544 722.938732 \nL 786.283499 720.224883 \nL 794.021557 718.309208 \nL 801.759616 715.59536 \nL 809.497674 712.163114 \nL 817.235733 712.242928 \nL 824.973791 712.642033 \nL 832.71185 708.970347 \nL 840.449909 703.702277 \nL 848.187967 702.185707 \nL 855.926026 700.669137 \nL 863.664084 700.27007 \nL 871.402143 700.589324 \nL 879.140201 700.589324 \nL 886.87826 700.27007 \nL 894.616318 703.462837 \nL 902.354377 708.411615 \nL 910.092435 712.242928 \nL 917.830494 716.074279 \nL 925.568553 722.779067 \nL 933.306611 728.126988 \nL 941.04467 728.206801 \nL 948.782728 725.812207 \nL 956.520787 726.291126 \nL 964.258845 727.887509 \nL 971.996904 727.887509 \nL 979.734962 724.37545 \nL 987.473021 720.623951 \nL 995.21108 716.792638 \n\" clip-path=\"url(#pe5db293d38)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_91\">\n    <path d=\"M 82.12017 789.422742 \nL 89.858229 789.705305 \nL 97.596288 790.046444 \nL 105.334346 790.523765 \nL 113.072405 791.15721 \nL 120.810463 791.6053 \nL 128.548522 792.014606 \nL 136.28658 792.267976 \nL 144.024639 792.48222 \nL 151.762697 792.764746 \nL 159.500756 793.144859 \nL 167.238815 793.534544 \nL 174.976873 793.79762 \nL 182.714932 794.031504 \nL 190.45299 794.060715 \nL 198.191049 794.245881 \nL 205.929107 794.616098 \nL 213.667166 794.966809 \nL 221.405224 795.307967 \nL 229.143283 795.600255 \nL 236.881341 795.7658 \nL 244.6194 795.833967 \nL 252.357459 795.843711 \nL 260.095517 795.707377 \nL 267.833576 795.736589 \nL 275.571634 796.009409 \nL 283.309693 796.233549 \nL 291.047751 796.321183 \nL 298.78581 796.467413 \nL 306.523868 796.564792 \nL 314.261927 796.506368 \nL 321.999986 796.350395 \nL 329.738044 796.360139 \nL 337.476103 796.40899 \nL 345.214161 796.496625 \nL 352.95222 796.496625 \nL 360.690278 796.40899 \nL 368.428337 796.233549 \nL 376.166395 796.058107 \nL 383.904454 795.98994 \nL 391.642513 796.106787 \nL 399.380571 796.282247 \nL 407.11863 796.311459 \nL 414.856688 796.291972 \nL 422.594747 796.447945 \nL 430.332805 796.584279 \nL 438.070864 796.594003 \nL 445.808922 796.603747 \nL 453.546981 796.740233 \nL 461.285039 796.866842 \nL 469.023098 796.983841 \nL 476.761157 797.139662 \nL 484.499215 797.022796 \nL 492.237274 796.88631 \nL 499.975332 796.642702 \nL 507.713391 796.311459 \nL 515.451449 795.941242 \nL 523.189508 795.678166 \nL 530.927566 795.775544 \nL 538.665625 795.950985 \nL 546.403684 796.253017 \nL 554.141742 796.477157 \nL 561.879801 796.447945 \nL 569.617859 796.379626 \nL 577.355918 796.564792 \nL 585.093976 796.711021 \nL 592.832035 796.749976 \nL 600.570093 796.711021 \nL 608.308152 796.75972 \nL 616.046211 796.701297 \nL 623.784269 796.40899 \nL 631.522328 795.873094 \nL 639.260386 795.26886 \nL 646.998445 794.898643 \nL 654.736503 794.460125 \nL 662.474562 793.934106 \nL 670.21262 793.44689 \nL 677.950679 793.242237 \nL 685.688737 793.368999 \nL 693.426796 793.690498 \nL 701.164855 793.79762 \nL 708.902913 793.310404 \nL 716.640972 792.910975 \nL 724.37903 792.599219 \nL 732.117089 791.751529 \nL 739.855147 790.845264 \nL 747.593206 790.406918 \nL 755.331264 790.221714 \nL 763.069323 790.036701 \nL 770.807382 789.851535 \nL 778.54544 789.763881 \nL 786.283499 789.715011 \nL 794.021557 789.549485 \nL 801.759616 789.325345 \nL 809.497674 789.237729 \nL 817.235733 789.403255 \nL 824.973791 789.412999 \nL 832.71185 789.549485 \nL 840.449909 789.656588 \nL 848.187967 789.598164 \nL 855.926026 789.549485 \nL 863.664084 789.627395 \nL 871.402143 789.715011 \nL 879.140201 790.143804 \nL 886.87826 790.494534 \nL 894.616318 790.747904 \nL 902.354377 790.962301 \nL 910.092435 791.040173 \nL 917.830494 791.05966 \nL 925.568553 790.962301 \nL 933.306611 790.874647 \nL 941.04467 790.669994 \nL 948.782728 790.562701 \nL 956.520787 790.280137 \nL 964.258845 790.075637 \nL 971.996904 790.075637 \nL 979.734962 790.260688 \nL 987.473021 790.582188 \nL 995.21108 790.93307 \n\" clip-path=\"url(#pe5db293d38)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_92\">\n    <path d=\"M 82.12017 786.918647 \nL 89.858229 786.090444 \nL 97.596288 785.174587 \nL 105.334346 784.278218 \nL 113.072405 783.284298 \nL 120.810463 782.290569 \nL 128.548522 781.47211 \nL 136.28658 780.760754 \nL 144.024639 780.17614 \nL 151.762697 779.688924 \nL 159.500756 779.357681 \nL 167.238815 778.997208 \nL 174.976873 778.685452 \nL 182.714932 778.432081 \nL 190.45299 778.217723 \nL 198.191049 778.188492 \nL 205.929107 778.432081 \nL 213.667166 778.860722 \nL 221.405224 779.240835 \nL 229.143283 779.562334 \nL 236.881341 779.688924 \nL 244.6194 779.620758 \nL 252.357459 779.474528 \nL 260.095517 779.357681 \nL 267.833576 779.396618 \nL 275.571634 779.747348 \nL 283.309693 780.166397 \nL 291.047751 780.751011 \nL 298.78581 781.34533 \nL 306.523868 781.929943 \nL 314.261927 782.456096 \nL 321.999986 782.99203 \nL 329.738044 783.479246 \nL 337.476103 783.927335 \nL 345.214161 784.336641 \nL 352.95222 784.872537 \nL 360.690278 785.486381 \nL 368.428337 786.236674 \nL 376.166395 787.035646 \nL 383.904454 787.844399 \nL 391.642513 788.653115 \nL 399.380571 789.364319 \nL 407.11863 790.017061 \nL 414.856688 790.66025 \nL 422.594747 791.2838 \nL 430.332805 791.985413 \nL 438.070864 792.725809 \nL 445.808922 793.534544 \nL 453.546981 794.323791 \nL 461.285039 795.220161 \nL 469.023098 796.126274 \nL 476.761157 796.964373 \nL 484.499215 797.665833 \nL 492.237274 798.357702 \nL 499.975332 798.990976 \nL 507.713391 799.585315 \nL 515.451449 800.160185 \nL 523.189508 800.881265 \nL 530.927566 801.611937 \nL 538.665625 802.245383 \nL 546.403684 802.693473 \nL 554.141742 802.976036 \nL 561.879801 803.132009 \nL 569.617859 803.170964 \nL 577.355918 803.151477 \nL 585.093976 803.170964 \nL 592.832035 803.180689 \nL 600.570093 803.102797 \nL 608.308152 802.907869 \nL 616.046211 802.596094 \nL 623.784269 802.13809 \nL 631.522328 801.777635 \nL 639.260386 801.728936 \nL 646.998445 801.86527 \nL 654.736503 801.953076 \nL 662.474562 802.021243 \nL 670.21262 801.992031 \nL 677.950679 801.845802 \nL 685.688737 801.621662 \nL 693.426796 801.436648 \nL 701.164855 801.358586 \nL 708.902913 801.270951 \nL 716.640972 801.124873 \nL 724.37903 801.056554 \nL 732.117089 801.31963 \nL 739.855147 801.456116 \nL 747.593206 801.563238 \nL 755.331264 801.748423 \nL 763.069323 801.81659 \nL 770.807382 801.728936 \nL 778.54544 801.680256 \nL 786.283499 801.582707 \nL 794.021557 801.456116 \nL 801.759616 801.339098 \nL 809.497674 801.163829 \nL 817.235733 800.793459 \nL 824.973791 800.423261 \nL 832.71185 800.199121 \nL 840.449909 799.867878 \nL 848.187967 799.361194 \nL 855.926026 798.805811 \nL 863.664084 798.201729 \nL 871.402143 797.587923 \nL 879.140201 796.935009 \nL 886.87826 796.311459 \nL 894.616318 795.678166 \nL 902.354377 795.035148 \nL 910.092435 794.596611 \nL 917.830494 794.304323 \nL 925.568553 794.041228 \nL 933.306611 793.924382 \nL 941.04467 794.060715 \nL 948.782728 794.197201 \nL 956.520787 794.187305 \nL 964.258845 794.099651 \nL 971.996904 794.099651 \nL 979.734962 794.138607 \nL 987.473021 794.050972 \nL 995.21108 793.865787 \n\" clip-path=\"url(#pe5db293d38)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_93\">\n    <path d=\"M 82.12017 781.481816 \nL 89.858229 781.637637 \nL 97.596288 781.861777 \nL 105.334346 782.280826 \nL 113.072405 782.816569 \nL 120.810463 783.371952 \nL 128.548522 783.946975 \nL 136.28658 784.512101 \nL 144.024639 785.125908 \nL 151.762697 785.788394 \nL 159.500756 786.441327 \nL 167.238815 786.99671 \nL 174.976873 787.52271 \nL 182.714932 788.097732 \nL 190.45299 788.662858 \nL 198.191049 789.286408 \nL 205.929107 790.007318 \nL 213.667166 790.855008 \nL 221.405224 791.87812 \nL 229.143283 793.018116 \nL 236.881341 794.080183 \nL 244.6194 795.142251 \nL 252.357459 796.087319 \nL 260.095517 796.847355 \nL 267.833576 797.568454 \nL 275.571634 798.289364 \nL 283.309693 799.029932 \nL 291.047751 799.828923 \nL 298.78581 800.647401 \nL 306.523868 801.504815 \nL 314.261927 802.381698 \nL 321.999986 803.239131 \nL 329.738044 803.969955 \nL 337.476103 804.505851 \nL 345.214161 804.944216 \nL 352.95222 805.441176 \nL 360.690278 805.986815 \nL 368.428337 806.512987 \nL 376.166395 806.941779 \nL 383.904454 807.29251 \nL 391.642513 807.438568 \nL 399.380571 807.311978 \nL 407.11863 806.970991 \nL 414.856688 806.493519 \nL 422.594747 805.908924 \nL 430.332805 805.236523 \nL 438.070864 804.457171 \nL 445.808922 803.619225 \nL 453.546981 802.937081 \nL 461.285039 802.352486 \nL 469.023098 801.826315 \nL 476.761157 801.397693 \nL 484.499215 800.939689 \nL 492.237274 800.491428 \nL 499.975332 800.004212 \nL 507.713391 799.42936 \nL 515.451449 798.7279 \nL 523.189508 797.88021 \nL 530.927566 796.847355 \nL 538.665625 795.561148 \nL 546.403684 794.255625 \nL 554.141742 793.105903 \nL 561.879801 791.917094 \nL 569.617859 790.825777 \nL 577.355918 790.007318 \nL 585.093976 789.412999 \nL 592.832035 789.033038 \nL 600.570093 788.896552 \nL 608.308152 788.896552 \nL 616.046211 789.042781 \nL 623.784269 789.286408 \nL 631.522328 789.510548 \nL 639.260386 789.588421 \nL 646.998445 789.627395 \nL 654.736503 789.568972 \nL 662.474562 789.520254 \nL 670.21262 789.344832 \nL 677.950679 789.062268 \nL 685.688737 788.877103 \nL 693.426796 788.896552 \nL 701.164855 789.013589 \nL 708.902913 789.218241 \nL 716.640972 789.442191 \nL 724.37903 789.968382 \nL 732.117089 790.338751 \nL 739.855147 790.611571 \nL 747.593206 790.864903 \nL 755.331264 791.049917 \nL 763.069323 790.894134 \nL 770.807382 790.533508 \nL 778.54544 790.026805 \nL 786.283499 789.403255 \nL 794.021557 788.925935 \nL 801.759616 788.721282 \nL 809.497674 788.653115 \nL 817.235733 788.604283 \nL 824.973791 788.545822 \nL 832.71185 788.692051 \nL 840.449909 789.003845 \nL 848.187967 789.335088 \nL 855.926026 789.637138 \nL 863.664084 789.880728 \nL 871.402143 790.065931 \nL 879.140201 790.299625 \nL 886.87826 790.562701 \nL 894.616318 790.855008 \nL 902.354377 791.351967 \nL 910.092435 791.936543 \nL 917.830494 792.579732 \nL 925.568553 793.203282 \nL 933.306611 793.719729 \nL 941.04467 794.089927 \nL 948.782728 794.304323 \nL 956.520787 794.382215 \nL 964.258845 794.314048 \nL 971.996904 794.14835 \nL 979.734962 793.817107 \nL 987.473021 793.466377 \nL 995.21108 793.105903 \n\" clip-path=\"url(#pe5db293d38)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 36.465625 853.918125 \nL 36.465625 673.135516 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path d=\"M 1040.865625 853.918125 \nL 1040.865625 673.135516 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 36.465625 853.918125 \nL 1040.865625 853.918125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 36.465625 673.135516 \nL 1040.865625 673.135516 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_73\">\n    <!-- Sample 72 Class:['AlohomoraCharm'] -->\n    <g transform=\"translate(426.4825 667.135516) scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-53\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"63.476562\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"124.755859\"/>\n     <use xlink:href=\"#DejaVuSans-70\" x=\"222.167969\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"285.644531\"/>\n     <use xlink:href=\"#DejaVuSans-65\" x=\"313.427734\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"374.951172\"/>\n     <use xlink:href=\"#DejaVuSans-37\" x=\"406.738281\"/>\n     <use xlink:href=\"#DejaVuSans-32\" x=\"470.361328\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"533.984375\"/>\n     <use xlink:href=\"#DejaVuSans-43\" x=\"565.771484\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"635.595703\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"663.378906\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"724.658203\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"776.757812\"/>\n     <use xlink:href=\"#DejaVuSans-3a\" x=\"828.857422\"/>\n     <use xlink:href=\"#DejaVuSans-5b\" x=\"862.548828\"/>\n     <use xlink:href=\"#DejaVuSans-27\" x=\"901.5625\"/>\n     <use xlink:href=\"#DejaVuSans-41\" x=\"929.052734\"/>\n     <use xlink:href=\"#DejaVuSans-6c\" x=\"997.460938\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1025.244141\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"1086.425781\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1149.804688\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1210.986328\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"1308.398438\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"1369.580078\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"1410.693359\"/>\n     <use xlink:href=\"#DejaVuSans-43\" x=\"1471.972656\"/>\n     <use xlink:href=\"#DejaVuSans-68\" x=\"1541.796875\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"1605.175781\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"1666.455078\"/>\n     <use xlink:href=\"#DejaVuSans-6d\" x=\"1705.818359\"/>\n     <use xlink:href=\"#DejaVuSans-27\" x=\"1803.230469\"/>\n     <use xlink:href=\"#DejaVuSans-5d\" x=\"1830.720703\"/>\n    </g>\n   </g>\n   <g id=\"legend_4\">\n    <g id=\"patch_25\">\n     <path d=\"M 988.667188 769.204266 \nL 1033.865625 769.204266 \nQ 1035.865625 769.204266 1035.865625 767.204266 \nL 1035.865625 680.135516 \nQ 1035.865625 678.135516 1033.865625 678.135516 \nL 988.667188 678.135516 \nQ 986.667188 678.135516 986.667188 680.135516 \nL 986.667188 767.204266 \nQ 986.667188 769.204266 988.667188 769.204266 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_94\">\n     <path d=\"M 990.667188 686.233954 \nL 1000.667188 686.233954 \nL 1010.667188 686.233954 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_74\">\n     <!-- aX -->\n     <g transform=\"translate(1018.667188 689.733954) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-58\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_95\">\n     <path d=\"M 990.667188 700.912079 \nL 1000.667188 700.912079 \nL 1010.667188 700.912079 \n\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_75\">\n     <!-- aY -->\n     <g transform=\"translate(1018.667188 704.412079) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-59\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_96\">\n     <path d=\"M 990.667188 715.590204 \nL 1000.667188 715.590204 \nL 1010.667188 715.590204 \n\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_76\">\n     <!-- aZ -->\n     <g transform=\"translate(1018.667188 719.090204) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use xlink:href=\"#DejaVuSans-5a\" x=\"61.279297\"/>\n     </g>\n    </g>\n    <g id=\"line2d_97\">\n     <path d=\"M 990.667188 730.268329 \nL 1000.667188 730.268329 \nL 1010.667188 730.268329 \n\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_77\">\n     <!-- gX -->\n     <g transform=\"translate(1018.667188 733.768329) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-58\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_98\">\n     <path d=\"M 990.667188 744.946454 \nL 1000.667188 744.946454 \nL 1010.667188 744.946454 \n\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_78\">\n     <!-- gY -->\n     <g transform=\"translate(1018.667188 748.446454) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-59\" x=\"63.476562\"/>\n     </g>\n    </g>\n    <g id=\"line2d_99\">\n     <path d=\"M 990.667188 759.624579 \nL 1000.667188 759.624579 \nL 1010.667188 759.624579 \n\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_79\">\n     <!-- gZ -->\n     <g transform=\"translate(1018.667188 763.124579) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-67\"/>\n      <use xlink:href=\"#DejaVuSans-5a\" x=\"63.476562\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4053fe7a9f\">\n   <rect x=\"36.465625\" y=\"22.318125\" width=\"1004.4\" height=\"180.782609\"/>\n  </clipPath>\n  <clipPath id=\"p30dc3d505c\">\n   <rect x=\"36.465625\" y=\"239.257255\" width=\"1004.4\" height=\"180.782609\"/>\n  </clipPath>\n  <clipPath id=\"pe778454874\">\n   <rect x=\"36.465625\" y=\"456.196386\" width=\"1004.4\" height=\"180.782609\"/>\n  </clipPath>\n  <clipPath id=\"pe5db293d38\">\n   <rect x=\"36.465625\" y=\"673.135516\" width=\"1004.4\" height=\"180.782609\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wuuieya0smzW"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model"
      ],
      "metadata": {
        "id": "dVqOOX3zA4aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleTimeSeriesCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(SimpleTimeSeriesCNN, self).__init__()\n",
        "\n",
        "        self.conv_block= nn.Sequential(\n",
        "            nn.Conv1d(in_channels=6, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            #\n",
        "            nn.Conv1d(in_channels=32, out_channels=32, kernel_size=3),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            #\n",
        "            nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(16 * 13, num_classes), # 59\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "CN9xfw6aA6Ob"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define training functions"
      ],
      "metadata": {
        "id": "DAJDan9OBGBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "def train(train_loader, model, criterion, optimizer, epoch, device):\n",
        "    model.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, \"training\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "def test(test_loader, model, criterion, optimizer, epoch, device):\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs,  labels in tqdm(test_loader, \"testing\"):\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          _, predicted = outputs.max(1)\n",
        "          test_total += labels.size(0)\n",
        "          test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "      test_accuracy = 100 * test_correct / test_total\n",
        "      avg_test_loss = test_loss / len(test_loader)\n",
        "      print(f\"Epoch {epoch+1}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\\n\")\n",
        "      return test_accuracy/100\n"
      ],
      "metadata": {
        "id": "Gy4hJZsgBGiG"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define helpers"
      ],
      "metadata": {
        "id": "2IVmZLC3qNdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "X0DG0rl_qPc5"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train models"
      ],
      "metadata": {
        "id": "wyATHdUiuV4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Z3iJLhPFuYMp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "epochs = 100\n",
        "\n",
        "set_seed(42)\n",
        "model = SimpleTimeSeriesCNN(num_classes=num_classes).to(device)\n",
        "print(model)\n",
        "print(f'The number of model parameters:{count_parameters(model)}')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "  train(train_loader, model, criterion, optimizer, epoch, device)\n",
        "  test_accuracy = test(test_loader, model, criterion, optimizer, epoch, device)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Running time: {elapsed_time} s\")\n",
        "\n",
        "print(f\"Test accuracy :{test_accuracy*100}%\")\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZPVLvYhBR92",
        "outputId": "267c6517-daba-4fe4-d387-20fc3d967d8d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleTimeSeriesCNN(\n",
            "  (conv_block): Sequential(\n",
            "    (0): Conv1d(6, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
            "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv1d(32, 16, kernel_size=(3,), stride=(1,))\n",
            "    (9): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=208, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "The number of model parameters:6853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 24.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 1.9429, Training Accuracy: 25.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 126.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Test Loss: 1.5386, Test Accuracy: 44.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 19.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training Loss: 1.6744, Training Accuracy: 31.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 91.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Test Loss: 1.4075, Test Accuracy: 60.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 18.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training Loss: 1.4586, Training Accuracy: 41.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 99.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Test Loss: 1.3066, Test Accuracy: 68.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 20.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training Loss: 1.2950, Training Accuracy: 60.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 151.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Test Loss: 1.2271, Test Accuracy: 68.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 24.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training Loss: 1.1708, Training Accuracy: 66.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 166.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Test Loss: 1.1619, Test Accuracy: 76.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 25.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training Loss: 1.0708, Training Accuracy: 67.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 161.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Test Loss: 1.1053, Test Accuracy: 72.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 25.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training Loss: 0.9862, Training Accuracy: 76.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 149.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Test Loss: 1.0528, Test Accuracy: 76.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 24.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training Loss: 0.9131, Training Accuracy: 82.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 148.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Test Loss: 1.0070, Test Accuracy: 76.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training Loss: 0.8501, Training Accuracy: 83.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 211.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Test Loss: 0.9685, Test Accuracy: 76.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 28.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training Loss: 0.7959, Training Accuracy: 84.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 157.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Test Loss: 0.9377, Test Accuracy: 76.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 26.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Training Loss: 0.7495, Training Accuracy: 85.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 187.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Test Loss: 0.9089, Test Accuracy: 76.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 25.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Training Loss: 0.7089, Training Accuracy: 86.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 181.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Test Loss: 0.8835, Test Accuracy: 80.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 19.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Training Loss: 0.6727, Training Accuracy: 87.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 170.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Test Loss: 0.8610, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Training Loss: 0.6403, Training Accuracy: 88.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 176.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Test Loss: 0.8408, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 28.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Training Loss: 0.6108, Training Accuracy: 89.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 155.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Test Loss: 0.8231, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Training Loss: 0.5837, Training Accuracy: 89.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 187.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Test Loss: 0.8069, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Training Loss: 0.5589, Training Accuracy: 89.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 186.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Test Loss: 0.7924, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Training Loss: 0.5364, Training Accuracy: 89.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 178.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Test Loss: 0.7804, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Training Loss: 0.5159, Training Accuracy: 89.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 185.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Test Loss: 0.7698, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 28.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Training Loss: 0.4964, Training Accuracy: 90.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 187.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Test Loss: 0.7604, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 26.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21, Training Loss: 0.4781, Training Accuracy: 90.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 178.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21, Test Loss: 0.7514, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22, Training Loss: 0.4609, Training Accuracy: 90.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 109.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22, Test Loss: 0.7424, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 19.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23, Training Loss: 0.4447, Training Accuracy: 90.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 183.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23, Test Loss: 0.7331, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 28.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24, Training Loss: 0.4292, Training Accuracy: 91.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 183.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24, Test Loss: 0.7235, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 29.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25, Training Loss: 0.4144, Training Accuracy: 91.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 184.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25, Test Loss: 0.7139, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 23.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26, Training Loss: 0.4003, Training Accuracy: 91.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 170.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26, Test Loss: 0.7050, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27, Training Loss: 0.3867, Training Accuracy: 91.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 107.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27, Test Loss: 0.6962, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 26.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28, Training Loss: 0.3739, Training Accuracy: 91.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 116.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28, Test Loss: 0.6871, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29, Training Loss: 0.3616, Training Accuracy: 91.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 155.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29, Test Loss: 0.6787, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 16.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Training Loss: 0.3500, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 82.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30, Test Loss: 0.6704, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 13.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31, Training Loss: 0.3390, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 96.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31, Test Loss: 0.6627, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 14.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32, Training Loss: 0.3284, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 83.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32, Test Loss: 0.6554, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 13.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33, Training Loss: 0.3184, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 87.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33, Test Loss: 0.6479, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 17.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34, Training Loss: 0.3092, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 222.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34, Test Loss: 0.6408, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 26.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35, Training Loss: 0.3007, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 189.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35, Test Loss: 0.6342, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 29.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36, Training Loss: 0.2925, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 183.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36, Test Loss: 0.6284, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37, Training Loss: 0.2845, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 166.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37, Test Loss: 0.6234, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 25.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38, Training Loss: 0.2770, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 184.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38, Test Loss: 0.6195, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 28.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39, Training Loss: 0.2697, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 164.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39, Test Loss: 0.6163, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 24.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Training Loss: 0.2628, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 165.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40, Test Loss: 0.6135, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 24.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41, Training Loss: 0.2562, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 196.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41, Test Loss: 0.6102, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42, Training Loss: 0.2499, Training Accuracy: 92.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 171.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42, Test Loss: 0.6068, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43, Training Loss: 0.2439, Training Accuracy: 93.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 136.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43, Test Loss: 0.6033, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 15.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44, Training Loss: 0.2380, Training Accuracy: 93.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 76.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44, Test Loss: 0.5995, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45, Training Loss: 0.2322, Training Accuracy: 94.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 62.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45, Test Loss: 0.5960, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46, Training Loss: 0.2266, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 126.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46, Test Loss: 0.5924, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 14.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47, Training Loss: 0.2212, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 134.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47, Test Loss: 0.5889, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48, Training Loss: 0.2160, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 96.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48, Test Loss: 0.5853, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 13.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49, Training Loss: 0.2109, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 89.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49, Test Loss: 0.5812, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 11.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Training Loss: 0.2059, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 80.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50, Test Loss: 0.5771, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 18.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51, Training Loss: 0.2011, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 180.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51, Test Loss: 0.5736, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 23.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52, Training Loss: 0.1964, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 158.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52, Test Loss: 0.5707, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 19.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53, Training Loss: 0.1919, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 130.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53, Test Loss: 0.5684, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 20.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54, Training Loss: 0.1875, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 198.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54, Test Loss: 0.5664, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 21.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55, Training Loss: 0.1832, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 124.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55, Test Loss: 0.5641, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 23.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56, Training Loss: 0.1790, Training Accuracy: 95.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 185.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56, Test Loss: 0.5607, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57, Training Loss: 0.1749, Training Accuracy: 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 121.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57, Test Loss: 0.5559, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 27.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58, Training Loss: 0.1709, Training Accuracy: 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 169.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58, Test Loss: 0.5506, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59, Training Loss: 0.1670, Training Accuracy: 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 86.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59, Test Loss: 0.5451, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 24.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Training Loss: 0.1631, Training Accuracy: 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 208.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60, Test Loss: 0.5400, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61, Training Loss: 0.1595, Training Accuracy: 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 108.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61, Test Loss: 0.5367, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62, Training Loss: 0.1560, Training Accuracy: 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 162.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62, Test Loss: 0.5352, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 26.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63, Training Loss: 0.1524, Training Accuracy: 96.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 178.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63, Test Loss: 0.5356, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 26.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64, Training Loss: 0.1490, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 94.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64, Test Loss: 0.5364, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65, Training Loss: 0.1457, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 64.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65, Test Loss: 0.5349, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66, Training Loss: 0.1424, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 80.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66, Test Loss: 0.5322, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67, Training Loss: 0.1389, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 49.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67, Test Loss: 0.5299, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 11.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68, Training Loss: 0.1357, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 84.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68, Test Loss: 0.5289, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 13.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69, Training Loss: 0.1325, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 169.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69, Test Loss: 0.5293, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 23.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70, Training Loss: 0.1294, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 77.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70, Test Loss: 0.5306, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 16.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71, Training Loss: 0.1263, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 98.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71, Test Loss: 0.5319, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72, Training Loss: 0.1233, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 105.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72, Test Loss: 0.5329, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 26.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73, Training Loss: 0.1204, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 120.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73, Test Loss: 0.5338, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 24.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74, Training Loss: 0.1175, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 128.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74, Test Loss: 0.5348, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 16.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75, Training Loss: 0.1146, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 76.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 75, Test Loss: 0.5354, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76, Training Loss: 0.1118, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 17.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 76, Test Loss: 0.5343, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 15.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77, Training Loss: 0.1089, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 111.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 77, Test Loss: 0.5349, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 14.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78, Training Loss: 0.1061, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 67.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 78, Test Loss: 0.5368, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79, Training Loss: 0.1034, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 94.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 79, Test Loss: 0.5365, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 21.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Training Loss: 0.1007, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 62.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80, Test Loss: 0.5354, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 19.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81, Training Loss: 0.0980, Training Accuracy: 97.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 95.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 81, Test Loss: 0.5352, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 21.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82, Training Loss: 0.0954, Training Accuracy: 98.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 95.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 82, Test Loss: 0.5335, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 19.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83, Training Loss: 0.0929, Training Accuracy: 99.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 89.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 83, Test Loss: 0.5325, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 20.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84, Training Loss: 0.0904, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 94.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 84, Test Loss: 0.5319, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 25.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85, Training Loss: 0.0879, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 53.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 85, Test Loss: 0.5323, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 18.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86, Training Loss: 0.0854, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 54.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 86, Test Loss: 0.5330, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 18.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87, Training Loss: 0.0830, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 176.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 87, Test Loss: 0.5333, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 18.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88, Training Loss: 0.0807, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 76.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 88, Test Loss: 0.5343, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 21.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89, Training Loss: 0.0784, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 108.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89, Test Loss: 0.5359, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 17.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90, Training Loss: 0.0760, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 76.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 90, Test Loss: 0.5364, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 22.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91, Training Loss: 0.0738, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 46.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 91, Test Loss: 0.5350, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 17.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92, Training Loss: 0.0717, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 48.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 92, Test Loss: 0.5347, Test Accuracy: 84.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 17.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93, Training Loss: 0.0696, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 114.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 93, Test Loss: 0.5370, Test Accuracy: 88.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 18.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94, Training Loss: 0.0676, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 86.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 94, Test Loss: 0.5392, Test Accuracy: 88.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 21.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95, Training Loss: 0.0656, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 73.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 95, Test Loss: 0.5406, Test Accuracy: 88.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96, Training Loss: 0.0637, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 131.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96, Test Loss: 0.5402, Test Accuracy: 88.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97, Training Loss: 0.0618, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 92.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97, Test Loss: 0.5402, Test Accuracy: 88.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 16.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98, Training Loss: 0.0600, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 43.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98, Test Loss: 0.5408, Test Accuracy: 88.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 23.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99, Training Loss: 0.0583, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 51.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99, Test Loss: 0.5410, Test Accuracy: 88.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training: 100%|██████████| 1/1 [00:00<00:00, 19.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Training Loss: 0.0565, Training Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "testing: 100%|██████████| 1/1 [00:00<00:00, 51.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, Test Loss: 0.5412, Test Accuracy: 88.00%\n",
            "\n",
            "Running time: 8.494128465652466 s\n",
            "Test accuracy :88.0%\n",
            "Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "muNxmPi6roqy"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todo: M5: https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html#define-the-network\n"
      ],
      "metadata": {
        "id": "snog5BsGNir1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model profile"
      ],
      "metadata": {
        "id": "vEMuhawBumWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "e6FYVeIb6Kwm"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        true_labels.extend(target.numpy())\n",
        "        predicted_labels.extend(predicted.numpy())\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "xbed6LwCsTok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0570fd7-eee8-493f-bbd3-f243db432452"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5 1 0 0 0]\n",
            " [2 5 0 0 0]\n",
            " [0 0 2 0 0]\n",
            " [0 0 0 6 0]\n",
            " [0 0 0 0 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
        "plt.ylabel('Ground Truth')\n",
        "plt.xlabel('Prediction')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig(\"confusion_matrix.png\", bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "K7EaRCJl6hZ1",
        "outputId": "70816fc4-a529-49c8-ba1c-a860fec887ec"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"551.82275pt\" height=\"447.954375pt\" viewBox=\"0 0 551.82275 447.954375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-01-03T10:43:38.467988</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 447.954375 \nL 551.82275 447.954375 \nL 551.82275 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 37.55625 410.398125 \nL 483.95625 410.398125 \nL 483.95625 22.318125 \nL 37.55625 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"QuadMesh_1\">\n    <path d=\"M 37.55625 22.318125 \nL 126.83625 22.318125 \nL 126.83625 99.934125 \nL 37.55625 99.934125 \nL 37.55625 22.318125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #f6ab83\"/>\n    <path d=\"M 126.83625 22.318125 \nL 216.11625 22.318125 \nL 216.11625 99.934125 \nL 126.83625 99.934125 \nL 126.83625 22.318125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #3f1b43\"/>\n    <path d=\"M 216.11625 22.318125 \nL 305.39625 22.318125 \nL 305.39625 99.934125 \nL 216.11625 99.934125 \nL 216.11625 22.318125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 305.39625 22.318125 \nL 394.67625 22.318125 \nL 394.67625 99.934125 \nL 305.39625 99.934125 \nL 305.39625 22.318125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 394.67625 22.318125 \nL 483.95625 22.318125 \nL 483.95625 99.934125 \nL 394.67625 99.934125 \nL 394.67625 22.318125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 37.55625 99.934125 \nL 126.83625 99.934125 \nL 126.83625 177.550125 \nL 37.55625 177.550125 \nL 37.55625 99.934125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #841e5a\"/>\n    <path d=\"M 126.83625 99.934125 \nL 216.11625 99.934125 \nL 216.11625 177.550125 \nL 126.83625 177.550125 \nL 126.83625 99.934125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #f6ab83\"/>\n    <path d=\"M 216.11625 99.934125 \nL 305.39625 99.934125 \nL 305.39625 177.550125 \nL 216.11625 177.550125 \nL 216.11625 99.934125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 305.39625 99.934125 \nL 394.67625 99.934125 \nL 394.67625 177.550125 \nL 305.39625 177.550125 \nL 305.39625 99.934125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 394.67625 99.934125 \nL 483.95625 99.934125 \nL 483.95625 177.550125 \nL 394.67625 177.550125 \nL 394.67625 99.934125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 37.55625 177.550125 \nL 126.83625 177.550125 \nL 126.83625 255.166125 \nL 37.55625 255.166125 \nL 37.55625 177.550125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 126.83625 177.550125 \nL 216.11625 177.550125 \nL 216.11625 255.166125 \nL 126.83625 255.166125 \nL 126.83625 177.550125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 216.11625 177.550125 \nL 305.39625 177.550125 \nL 305.39625 255.166125 \nL 216.11625 255.166125 \nL 216.11625 177.550125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #841e5a\"/>\n    <path d=\"M 305.39625 177.550125 \nL 394.67625 177.550125 \nL 394.67625 255.166125 \nL 305.39625 255.166125 \nL 305.39625 177.550125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 394.67625 177.550125 \nL 483.95625 177.550125 \nL 483.95625 255.166125 \nL 394.67625 255.166125 \nL 394.67625 177.550125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 37.55625 255.166125 \nL 126.83625 255.166125 \nL 126.83625 332.782125 \nL 37.55625 332.782125 \nL 37.55625 255.166125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 126.83625 255.166125 \nL 216.11625 255.166125 \nL 216.11625 332.782125 \nL 126.83625 332.782125 \nL 126.83625 255.166125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 216.11625 255.166125 \nL 305.39625 255.166125 \nL 305.39625 332.782125 \nL 216.11625 332.782125 \nL 216.11625 255.166125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 305.39625 255.166125 \nL 394.67625 255.166125 \nL 394.67625 332.782125 \nL 305.39625 332.782125 \nL 305.39625 255.166125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #faebdd\"/>\n    <path d=\"M 394.67625 255.166125 \nL 483.95625 255.166125 \nL 483.95625 332.782125 \nL 394.67625 332.782125 \nL 394.67625 255.166125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 37.55625 332.782125 \nL 126.83625 332.782125 \nL 126.83625 410.398125 \nL 37.55625 410.398125 \nL 37.55625 332.782125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 126.83625 332.782125 \nL 216.11625 332.782125 \nL 216.11625 410.398125 \nL 126.83625 410.398125 \nL 126.83625 332.782125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 216.11625 332.782125 \nL 305.39625 332.782125 \nL 305.39625 410.398125 \nL 216.11625 410.398125 \nL 216.11625 332.782125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 305.39625 332.782125 \nL 394.67625 332.782125 \nL 394.67625 410.398125 \nL 305.39625 410.398125 \nL 305.39625 332.782125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #03051a\"/>\n    <path d=\"M 394.67625 332.782125 \nL 483.95625 332.782125 \nL 483.95625 410.398125 \nL 394.67625 410.398125 \nL 394.67625 332.782125 \n\" clip-path=\"url(#p2e96b1b9dd)\" style=\"fill: #f06043\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"me04aeeca99\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#me04aeeca99\" x=\"82.19625\" y=\"410.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(79.015 424.996562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#me04aeeca99\" x=\"171.47625\" y=\"410.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(168.295 424.996562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#me04aeeca99\" x=\"260.75625\" y=\"410.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(257.575 424.996562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#me04aeeca99\" x=\"350.03625\" y=\"410.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(346.855 424.996562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#me04aeeca99\" x=\"439.31625\" y=\"410.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 4 -->\n      <g transform=\"translate(436.135 424.996562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- Prediction -->\n     <g transform=\"translate(235.919531 438.674687) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-50\" d=\"M 1259 4147 \nL 1259 2394 \nL 2053 2394 \nQ 2494 2394 2734 2622 \nQ 2975 2850 2975 3272 \nQ 2975 3691 2734 3919 \nQ 2494 4147 2053 4147 \nL 1259 4147 \nz\nM 628 4666 \nL 2053 4666 \nQ 2838 4666 3239 4311 \nQ 3641 3956 3641 3272 \nQ 3641 2581 3239 2228 \nQ 2838 1875 2053 1875 \nL 1259 1875 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-50\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"58.552734\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"97.416016\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"158.939453\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"222.416016\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"250.199219\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"305.179688\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"344.388672\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"372.171875\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"433.353516\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path id=\"m6658e0810d\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m6658e0810d\" x=\"37.55625\" y=\"61.126125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(28.476562 64.307375) rotate(-90) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m6658e0810d\" x=\"37.55625\" y=\"138.742125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1 -->\n      <g transform=\"translate(28.476562 141.923375) rotate(-90) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m6658e0810d\" x=\"37.55625\" y=\"216.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2 -->\n      <g transform=\"translate(28.476562 219.539375) rotate(-90) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m6658e0810d\" x=\"37.55625\" y=\"293.974125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 3 -->\n      <g transform=\"translate(28.476562 297.155375) rotate(-90) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m6658e0810d\" x=\"37.55625\" y=\"371.590125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 4 -->\n      <g transform=\"translate(28.476562 374.771375) rotate(-90) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- Ground Truth -->\n     <g transform=\"translate(14.798437 249.005) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-47\" d=\"M 3809 666 \nL 3809 1919 \nL 2778 1919 \nL 2778 2438 \nL 4434 2438 \nL 4434 434 \nQ 4069 175 3628 42 \nQ 3188 -91 2688 -91 \nQ 1594 -91 976 548 \nQ 359 1188 359 2328 \nQ 359 3472 976 4111 \nQ 1594 4750 2688 4750 \nQ 3144 4750 3555 4637 \nQ 3966 4525 4313 4306 \nL 4313 3634 \nQ 3963 3931 3569 4081 \nQ 3175 4231 2741 4231 \nQ 1884 4231 1454 3753 \nQ 1025 3275 1025 2328 \nQ 1025 1384 1454 906 \nQ 1884 428 2741 428 \nQ 3075 428 3337 486 \nQ 3600 544 3809 666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-54\" d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-47\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"77.490234\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"116.353516\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"177.535156\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"240.914062\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"304.292969\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"367.769531\"/>\n      <use xlink:href=\"#DejaVuSans-54\" x=\"399.556641\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"445.890625\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"487.003906\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"550.382812\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"589.591797\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_13\">\n    <!-- 5 -->\n    <g style=\"fill: #262626\" transform=\"translate(79.015 63.8855) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-35\"/>\n    </g>\n   </g>\n   <g id=\"text_14\">\n    <!-- 1 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(168.295 63.8855) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-31\"/>\n    </g>\n   </g>\n   <g id=\"text_15\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(257.575 63.8855) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_16\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(346.855 63.8855) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_17\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(436.135 63.8855) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_18\">\n    <!-- 2 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(79.015 141.5015) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-32\"/>\n    </g>\n   </g>\n   <g id=\"text_19\">\n    <!-- 5 -->\n    <g style=\"fill: #262626\" transform=\"translate(168.295 141.5015) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-35\"/>\n    </g>\n   </g>\n   <g id=\"text_20\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(257.575 141.5015) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_21\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(346.855 141.5015) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_22\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(436.135 141.5015) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_23\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(79.015 219.1175) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_24\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(168.295 219.1175) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_25\">\n    <!-- 2 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(257.575 219.1175) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-32\"/>\n    </g>\n   </g>\n   <g id=\"text_26\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(346.855 219.1175) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_27\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(436.135 219.1175) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_28\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(79.015 296.7335) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_29\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(168.295 296.7335) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_30\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(257.575 296.7335) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_31\">\n    <!-- 6 -->\n    <g style=\"fill: #262626\" transform=\"translate(346.855 296.7335) scale(0.1 -0.1)\">\n     <defs>\n      <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-36\"/>\n    </g>\n   </g>\n   <g id=\"text_32\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(436.135 296.7335) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_33\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(79.015 374.3495) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_34\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(168.295 374.3495) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_35\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(257.575 374.3495) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_36\">\n    <!-- 0 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(346.855 374.3495) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-30\"/>\n    </g>\n   </g>\n   <g id=\"text_37\">\n    <!-- 4 -->\n    <g style=\"fill: #ffffff\" transform=\"translate(436.135 374.3495) scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-34\"/>\n    </g>\n   </g>\n   <g id=\"text_38\">\n    <!-- Confusion Matrix -->\n    <g transform=\"translate(210.114375 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-66\" d=\"M 2375 4863 \nL 2375 4384 \nL 1825 4384 \nQ 1516 4384 1395 4259 \nQ 1275 4134 1275 3809 \nL 1275 3500 \nL 2222 3500 \nL 2222 3053 \nL 1275 3053 \nL 1275 0 \nL 697 0 \nL 697 3053 \nL 147 3053 \nL 147 3500 \nL 697 3500 \nL 697 3744 \nQ 697 4328 969 4595 \nQ 1241 4863 1831 4863 \nL 2375 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-4d\" d=\"M 628 4666 \nL 1569 4666 \nL 2759 1491 \nL 3956 4666 \nL 4897 4666 \nL 4897 0 \nL 4281 0 \nL 4281 4097 \nL 3078 897 \nL 2444 897 \nL 1241 4097 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-43\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"69.824219\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"131.005859\"/>\n     <use xlink:href=\"#DejaVuSans-66\" x=\"194.384766\"/>\n     <use xlink:href=\"#DejaVuSans-75\" x=\"229.589844\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"292.96875\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"345.068359\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"372.851562\"/>\n     <use xlink:href=\"#DejaVuSans-6e\" x=\"434.033203\"/>\n     <use xlink:href=\"#DejaVuSans-20\" x=\"497.412109\"/>\n     <use xlink:href=\"#DejaVuSans-4d\" x=\"529.199219\"/>\n     <use xlink:href=\"#DejaVuSans-61\" x=\"615.478516\"/>\n     <use xlink:href=\"#DejaVuSans-74\" x=\"676.757812\"/>\n     <use xlink:href=\"#DejaVuSans-72\" x=\"715.966797\"/>\n     <use xlink:href=\"#DejaVuSans-69\" x=\"757.080078\"/>\n     <use xlink:href=\"#DejaVuSans-78\" x=\"784.863281\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_3\">\n    <path d=\"M 511.85625 410.398125 \nL 531.26025 410.398125 \nL 531.26025 22.318125 \nL 511.85625 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <image xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAABsAAAIbCAYAAAAaS+V8AAACgElEQVR4nO2di23DUBDD/NLXJbr/nu0QJQSYIBc4yNJ96hTJ+fr++X1GfFaFnud57hkWGys7O20pe1+xex7pY/QG5H60yrxpFHumVfbRplHsmVVZAUGoqRE6CxC840qcxiYIQZ4h3HN29cwHjzYg4hWTZwB5hpBnCHmG4PWsuxGhWx8hzxC846o0IuQZgliZ9n94uogRWjEIrRgEs2daZctq9RmCt89qagRxU2ujrx5XO5ogCOI+K40EpfF9xbyvk8RveBpXBHmGkGcILU8E7z4rIAjipm4QEzSuEJogCKURoTQiiA+eok8wVjb7mqaaGkLdZzuaIAilEUF8FnhXTNEnSBlCfYZQGhHEyo62z7Se9RgRWjEIpRGhNCKs07gzrTQieCdITY1wP7/aph4WK40IrRiEdRq1ng2Lpex9xYo+Qp4hmJfn8OIpjQjmCTL8FGHdZ1Zl2j+ZCghCTY2w9mxXrDQilEaE0ojgTeM4IMO3mzU1gtozqzLtuLrDX7cqjQzqCbIrlmfvK+Z9jEUfoeWJoF6eu2L1GYLYs8faZwUEIWUI46bWKsszgjxDUHtm/eYTrWcFBMEbkPtoPSuNBKURQZ3GlP0f81mg/e0RbUDUynb18gyh5Ymw/hChPvs/KXtfsXtaMQCNKwSzZ/UZQH2GIPZMu88KCEJNjdDyRLiP+H2j1jOxMm0axVNfqqzXtghFHyFlCB08CE0QBLNnVmWNKwRvQMzKGsQAeYaQZwh5huBVVvQRvAHJM4Q8QxB7ds7XrFieIXiVNa4QvAFRe6YdxGLPtMq0nnXwEJTG9xUrIAidBQhqz6zKTp4B1GcI3tlYQBDETd0gJqjPELyD+A8x2y/1UHxA+AAAAABJRU5ErkJggg==\" id=\"imagef3be2ca929\" transform=\"scale(1 -1) translate(0 -388.08)\" x=\"511.92\" y=\"-22.32\" width=\"19.44\" height=\"388.08\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path id=\"m7d5e7bf87f\" d=\"M 0 0 \nL 3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m7d5e7bf87f\" x=\"531.26025\" y=\"410.398125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_39\">\n      <!-- 0 -->\n      <g transform=\"translate(538.26025 414.197344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m7d5e7bf87f\" x=\"531.26025\" y=\"345.718125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_40\">\n      <!-- 1 -->\n      <g transform=\"translate(538.26025 349.517344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#m7d5e7bf87f\" x=\"531.26025\" y=\"281.038125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_41\">\n      <!-- 2 -->\n      <g transform=\"translate(538.26025 284.837344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m7d5e7bf87f\" x=\"531.26025\" y=\"216.358125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_42\">\n      <!-- 3 -->\n      <g transform=\"translate(538.26025 220.157344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_15\">\n      <g>\n       <use xlink:href=\"#m7d5e7bf87f\" x=\"531.26025\" y=\"151.678125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_43\">\n      <!-- 4 -->\n      <g transform=\"translate(538.26025 155.477344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m7d5e7bf87f\" x=\"531.26025\" y=\"86.998125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_44\">\n      <!-- 5 -->\n      <g transform=\"translate(538.26025 90.797344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_17\">\n      <g>\n       <use xlink:href=\"#m7d5e7bf87f\" x=\"531.26025\" y=\"22.318125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_45\">\n      <!-- 6 -->\n      <g transform=\"translate(538.26025 26.117344) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\"/>\n   <g id=\"patch_4\">\n    <path d=\"M 511.85625 410.398125 \nL 521.55825 410.398125 \nL 531.26025 410.398125 \nL 531.26025 22.318125 \nL 521.55825 22.318125 \nL 511.85625 22.318125 \nL 511.85625 410.398125 \nz\n\" style=\"fill: none\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p2e96b1b9dd\">\n   <rect x=\"37.55625\" y=\"22.318125\" width=\"446.4\" height=\"388.08\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from calflops import calculate_flops\n",
        "batch_size = 1\n",
        "input_shape = (batch_size, 6, 119)\n",
        "flops, macs, params = calculate_flops(model=model,\n",
        "                                      input_shape=input_shape,\n",
        "                                      output_as_string=True,\n",
        "                                      output_precision=4)\n",
        "print(\"Model FLOPs:%s   MACs:%s   Params:%s \\n\" %(flops, macs, params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgMnf8VOsZ75",
        "outputId": "71a77a36-7984-4ff1-dd35-646d15580460"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------- Calculate Flops Results -------------------------------------\n",
            "Notations:\n",
            "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
            "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
            "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
            "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
            "\n",
            "Total Training Params:                                                  6.85 K  \n",
            "fwd MACs:                                                               330.32 KMACs\n",
            "fwd FLOPs:                                                              690.88 KFLOPS\n",
            "fwd+bwd MACs:                                                           990.96 KMACs\n",
            "fwd+bwd FLOPs:                                                          2.0726 MFLOPS\n",
            "\n",
            "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
            "Each module caculated is listed after its name in the following order: \n",
            "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
            "\n",
            "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
            " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
            "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
            "\n",
            "SimpleTimeSeriesCNN(\n",
            "  6.85 K = 100% Params, 330.32 KMACs = 100% MACs, 690.88 KFLOPS = 47.8115% FLOPs\n",
            "  (conv_block): Sequential(\n",
            "    5.81 K = 84.7512% Params, 329.28 KMACs = 99.6852% MACs, 688.8 KFLOPS = 47.661% FLOPs\n",
            "    (0): Conv1d(992 = 14.4754% Params, 114.24 KMACs = 34.5846% MACs, 232.29 KFLOPS = 16.5354% FLOPs, 6, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "    (1): BatchNorm1d(64 = 0.9339% Params, 0 MACs = 0% MACs, 7.62 KFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 3.81 KFLOPS = 0% FLOPs)\n",
            "    (3): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 3.81 KFLOPS = 0% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv1d(3.1 K = 45.294% Params, 175.1 KMACs = 53.0104% MACs, 352.03 KFLOPS = 25.3451% FLOPs, 32, 32, kernel_size=(3,), stride=(1,))\n",
            "    (5): BatchNorm1d(64 = 0.9339% Params, 0 MACs = 0% MACs, 3.65 KFLOPS = 0% FLOPs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 1.82 KFLOPS = 0% FLOPs)\n",
            "    (7): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 1.82 KFLOPS = 0% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv1d(1.55 K = 22.647% Params, 39.94 KMACs = 12.0901% MACs, 80.29 KFLOPS = 5.7805% FLOPs, 32, 16, kernel_size=(3,), stride=(1,))\n",
            "    (9): BatchNorm1d(32 = 0.4669% Params, 0 MACs = 0% MACs, 832 FLOPS = 0% FLOPs, 16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(0 = 0% Params, 0 MACs = 0% MACs, 416 FLOPS = 0% FLOPs)\n",
            "    (11): MaxPool1d(0 = 0% Params, 0 MACs = 0% MACs, 416 FLOPS = 0% FLOPs, kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    1.04 K = 15.2488% Params, 1.04 KMACs = 0.3148% MACs, 2.08 KFLOPS = 0.1505% FLOPs\n",
            "    (0): Linear(1.04 K = 15.2488% Params, 1.04 KMACs = 0.3148% MACs, 2.08 KFLOPS = 0.1505% FLOPs, in_features=208, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "---------------------------------------------------------------------------------------------------\n",
            "Model FLOPs:690.88 KFLOPS   MACs:330.32 KMACs   Params:6.853 K \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Conversion : pytorch->ONNX->TFLite"
      ],
      "metadata": {
        "id": "eriA9AvFyaIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnx import helper\n",
        "from onnx_tf.backend import prepare\n",
        "# import tensorflow as tf"
      ],
      "metadata": {
        "id": "nGmiynT-zbkv"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 6, 119, dtype=torch.float32)\n",
        "torch.onnx.export(\n",
        "        model=model.cpu(),\n",
        "        args=dummy_input,\n",
        "        f=\"model.onnx\",\n",
        "        verbose=False,\n",
        "        export_params=True,\n",
        "        do_constant_folding=False,\n",
        "        input_names=['input'],\n",
        "        opset_version=11,\n",
        "        output_names=['output'])\n",
        "print('ONNX model successfully converted')\n",
        "onnx_model = onnx.load(\"model.onnx\")\n",
        "\n",
        "# add softmax\n",
        "last_node_output = onnx_model.graph.node[-1].output[0]\n",
        "softmax_input = [last_node_output]\n",
        "\n",
        "softmax_output = last_node_output + \"_softmax\"\n",
        "softmax_node = helper.make_node(\n",
        "    \"Softmax\",\n",
        "    inputs=softmax_input,\n",
        "    outputs=[softmax_output],\n",
        "    axis=1\n",
        ")\n",
        "onnx_model.graph.node.append(softmax_node)\n",
        "onnx_model.graph.output[0].name = softmax_output\n",
        "onnx.save(onnx_model, \"model.onnx\")\n",
        "# add softmax\n",
        "\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# tf_rep = prepare(onnx_model)\n",
        "# tf_rep.export_graph(\"model.pb\")\n",
        "# print('TensorFlow model successfully converted')"
      ],
      "metadata": {
        "id": "xF47ZK-Wzd_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df3a7e9-7b15-4ca5-881e-55a2a2655db3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model successfully converted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onnx2tf -i model.onnx  -v info"
      ],
      "metadata": {
        "id": "p2vx6E8Pzuf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2eefd8-8fba-40cf-e7a6-62fbcb9bd773"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
            "Simplifying...\n",
            "Finish! Here is the difference:\n",
            "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
            "┃                    ┃ Original Model ┃ Simplified Model ┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
            "│ BatchNormalization │ 3              │ 0                │\n",
            "│ Constant           │ 20             │ 8                │\n",
            "│ Conv               │ 3              │ 3                │\n",
            "│ Flatten            │ 1              │ 1                │\n",
            "│ Gemm               │ 1              │ 1                │\n",
            "│ MaxPool            │ 3              │ 3                │\n",
            "│ Relu               │ 3              │ 3                │\n",
            "│ Softmax            │ 1              │ 1                │\n",
            "│ Model Size         │ 30.9KiB        │ 28.8KiB          │\n",
            "└────────────────────┴────────────────┴──────────────────┘\n",
            "\n",
            "Simplifying...\n",
            "Finish! Here is the difference:\n",
            "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
            "┃            ┃ Original Model ┃ Simplified Model ┃\n",
            "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
            "│ Constant   │ 8              │ 8                │\n",
            "│ Conv       │ 3              │ 3                │\n",
            "│ Flatten    │ 1              │ 1                │\n",
            "│ Gemm       │ 1              │ 1                │\n",
            "│ MaxPool    │ 3              │ 3                │\n",
            "│ Relu       │ 3              │ 3                │\n",
            "│ Softmax    │ 1              │ 1                │\n",
            "│ Model Size │ 28.8KiB        │ 28.8KiB          │\n",
            "└────────────┴────────────────┴──────────────────┘\n",
            "\n",
            "Simplifying...\n",
            "Finish! Here is the difference:\n",
            "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
            "┃            ┃ Original Model ┃ Simplified Model ┃\n",
            "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
            "│ Constant   │ 8              │ 8                │\n",
            "│ Conv       │ 3              │ 3                │\n",
            "│ Flatten    │ 1              │ 1                │\n",
            "│ Gemm       │ 1              │ 1                │\n",
            "│ MaxPool    │ 3              │ 3                │\n",
            "│ Relu       │ 3              │ 3                │\n",
            "│ Softmax    │ 1              │ 1                │\n",
            "│ Model Size │ 28.8KiB        │ 28.8KiB          │\n",
            "└────────────┴────────────────┴──────────────────┘\n",
            "\n",
            "\u001b[32mModel optimizing complete!\u001b[0m\n",
            "\n",
            "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
            "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
            "\n",
            "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
            "\n",
            "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
            "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
            "\u001b[32msaved_model output complete!\u001b[0m\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 12, Total Ops 27, % non-converted = 44.44 %\n",
            " * 12 ARITH ops\n",
            "\n",
            "- arith.constant:   12 occurrences  (f32: 8, i32: 4)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 1)\n",
            "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 12, Total Ops 35, % non-converted = 34.29 %\n",
            " * 12 ARITH ops\n",
            "\n",
            "- arith.constant:   12 occurrences  (f16: 8, i32: 4)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 3)\n",
            "  (f32: 8)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 1)\n",
            "\u001b[32mFloat16 tflite output complete!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp saved_model/model_float32.tflite model.tflite"
      ],
      "metadata": {
        "id": "NgK8E1xz0WVR"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!xxd -i model.tflite > model.h"
      ],
      "metadata": {
        "id": "T1MvOJkFz2fT"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n1 model.h"
      ],
      "metadata": {
        "id": "1za2LoKk0fMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176c315e-0671-4d15-d726-869c7580b0d5"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unsigned int model_tflite_len = 31024;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFLite float32 model evaluation"
      ],
      "metadata": {
        "id": "G6Jp3zNz14Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "\n",
        "def test_tflite_on_onebatch(inputs, labels):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "      # test_sample = np.array(X_test[i][:3], dtype=np.float32).reshape(input_shape)\n",
        "      # expected_output = y_test[i]\n",
        "      test_sample = inputs[i].unsqueeze(0)\n",
        "      expected_output = labels[i]\n",
        "\n",
        "      interpreter.set_tensor(input_details[0]['index'], test_sample)\n",
        "      interpreter.invoke()\n",
        "      output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "      predicted_output = np.argmax(output_data)\n",
        "\n",
        "      if predicted_output == expected_output:\n",
        "          correct_predictions += 1\n",
        "      total_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    # print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "ts_list=[]\n",
        "for inputs, labels in test_loader:\n",
        "  test_accuracy = test_tflite_on_onebatch(inputs.permute(0,2,1), labels)\n",
        "  ts_list.append(test_accuracy)\n",
        "\n",
        "print(f'Test accutacy of TFLite model :{np.mean(ts_list)*100}%')\n",
        "\n"
      ],
      "metadata": {
        "id": "k2aS78Zr16md",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f634b20-6795-4bfc-8528-3ce523c383be"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accutacy of TFLite model :88.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AVG Runtime:114.18704 milliseconds\n",
        "\n",
        "RAM Usage: 24.3K\n",
        "Flash Usage: 49.9K"
      ],
      "metadata": {
        "id": "PbXDW_Jtaaue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization: ONNX->TFKeras->Quantizaed TFLite"
      ],
      "metadata": {
        "id": "lqWpdzMNPlh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!onnx2tf -i model.onnx -oh5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi5Woxu7UF84",
        "outputId": "a318d7d5-68d5-4129-f6d3-6c402d726062"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[07mModel optimizing started\u001b[0m ============================================================\n",
            "Simplifying...\n",
            "Finish! Here is the difference:\n",
            "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
            "┃            ┃ Original Model ┃ Simplified Model ┃\n",
            "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
            "│ Constant   │ 8              │ 8                │\n",
            "│ Conv       │ 3              │ 3                │\n",
            "│ Flatten    │ 1              │ 1                │\n",
            "│ Gemm       │ 1              │ 1                │\n",
            "│ MaxPool    │ 3              │ 3                │\n",
            "│ Relu       │ 3              │ 3                │\n",
            "│ Softmax    │ 1              │ 1                │\n",
            "│ Model Size │ 28.8KiB        │ 28.8KiB          │\n",
            "└────────────┴────────────────┴──────────────────┘\n",
            "\n",
            "Simplifying...\n",
            "Finish! Here is the difference:\n",
            "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
            "┃            ┃ Original Model ┃ Simplified Model ┃\n",
            "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
            "│ Constant   │ 8              │ 8                │\n",
            "│ Conv       │ 3              │ 3                │\n",
            "│ Flatten    │ 1              │ 1                │\n",
            "│ Gemm       │ 1              │ 1                │\n",
            "│ MaxPool    │ 3              │ 3                │\n",
            "│ Relu       │ 3              │ 3                │\n",
            "│ Softmax    │ 1              │ 1                │\n",
            "│ Model Size │ 28.8KiB        │ 28.8KiB          │\n",
            "└────────────┴────────────────┴──────────────────┘\n",
            "\n",
            "Simplifying...\n",
            "Finish! Here is the difference:\n",
            "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
            "┃            ┃ Original Model ┃ Simplified Model ┃\n",
            "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
            "│ Constant   │ 8              │ 8                │\n",
            "│ Conv       │ 3              │ 3                │\n",
            "│ Flatten    │ 1              │ 1                │\n",
            "│ Gemm       │ 1              │ 1                │\n",
            "│ MaxPool    │ 3              │ 3                │\n",
            "│ Relu       │ 3              │ 3                │\n",
            "│ Softmax    │ 1              │ 1                │\n",
            "│ Model Size │ 28.8KiB        │ 28.8KiB          │\n",
            "└────────────┴────────────────┴──────────────────┘\n",
            "\n",
            "\u001b[32mModel optimizing complete!\u001b[0m\n",
            "\n",
            "\u001b[07mAutomatic generation of each OP name started\u001b[0m ========================================\n",
            "\u001b[32mAutomatic generation of each OP name complete!\u001b[0m\n",
            "\n",
            "\u001b[07mModel loaded\u001b[0m ========================================================================\n",
            "\n",
            "\u001b[07mModel conversion started\u001b[0m ============================================================\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32minput_op_name\u001b[0m: input \u001b[32mshape\u001b[0m: [1, 6, 119] \u001b[32mdtype\u001b[0m: float32\n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m2 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Conv\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.0/Conv\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: input \u001b[36mshape\u001b[0m: [1, 6, 119] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: 54 \u001b[36mshape\u001b[0m: [32, 6, 5] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: 60 \u001b[36mshape\u001b[0m: [32] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.0/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 32, 119] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: convolution_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: input \u001b[34mshape\u001b[0m: (1, 119, 6) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.weights\u001b[0m: \u001b[34mshape\u001b[0m: (5, 6, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.bias\u001b[0m: \u001b[34mshape\u001b[0m: (32,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: SAME \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.group\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add/Add:0 \u001b[34mshape\u001b[0m: (1, 119, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m3 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Relu\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.2/Relu\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.0/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 32, 119] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.2/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 32, 119] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: relu\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add/Add:0 \u001b[34mshape\u001b[0m: (1, 119, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu/Relu:0 \u001b[34mshape\u001b[0m: (1, 119, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m4 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: MaxPool\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.3/MaxPool\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.2/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 32, 119] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.3/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 32, 59] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: max_pool_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu/Relu:0 \u001b[34mshape\u001b[0m: (1, 119, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.filters\u001b[0m: \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.kernel_shape\u001b[0m: \u001b[34mval\u001b[0m: [2] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [2] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: [0, 0] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.ceil_mode\u001b[0m: \u001b[34mval\u001b[0m: False \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output0\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.max_pool1d/MaxPool1d/Squeeze:0 \u001b[34mshape\u001b[0m: (1, 59, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.2.output1\u001b[0m: \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m5 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Conv\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.4/Conv\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.3/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 32, 59] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: 78 \u001b[36mshape\u001b[0m: [32, 32, 3] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: 84 \u001b[36mshape\u001b[0m: [32] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.4/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 32, 57] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: convolution_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.max_pool1d/MaxPool1d/Squeeze:0 \u001b[34mshape\u001b[0m: (1, 59, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.weights\u001b[0m: \u001b[34mshape\u001b[0m: (3, 32, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.bias\u001b[0m: \u001b[34mshape\u001b[0m: (32,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: VALID \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.group\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (1, 57, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m6 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Relu\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.6/Relu\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.4/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 32, 57] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.6/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 32, 57] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: relu\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_1/Add:0 \u001b[34mshape\u001b[0m: (1, 57, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu_1/Relu:0 \u001b[34mshape\u001b[0m: (1, 57, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m7 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: MaxPool\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.7/MaxPool\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.6/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 32, 57] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.7/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 32, 28] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: max_pool_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu_1/Relu:0 \u001b[34mshape\u001b[0m: (1, 57, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.filters\u001b[0m: \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.kernel_shape\u001b[0m: \u001b[34mval\u001b[0m: [2] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [2] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: [0, 0] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.ceil_mode\u001b[0m: \u001b[34mval\u001b[0m: False \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output0\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.max_pool1d_1/MaxPool1d/Squeeze:0 \u001b[34mshape\u001b[0m: (1, 28, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.2.output1\u001b[0m: \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m8 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Conv\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.8/Conv\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.7/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 32, 28] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: 102 \u001b[36mshape\u001b[0m: [16, 32, 3] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: 108 \u001b[36mshape\u001b[0m: [16] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.8/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 16, 26] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: convolution_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.max_pool1d_1/MaxPool1d/Squeeze:0 \u001b[34mshape\u001b[0m: (1, 28, 32) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.weights\u001b[0m: \u001b[34mshape\u001b[0m: (3, 32, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.bias\u001b[0m: \u001b[34mshape\u001b[0m: (16,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: VALID \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.group\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_2/Add:0 \u001b[34mshape\u001b[0m: (1, 26, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m9 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Relu\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.10/Relu\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.8/Conv_output_0 \u001b[36mshape\u001b[0m: [1, 16, 26] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.10/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 16, 26] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: relu\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.features\u001b[0m: \u001b[34mname\u001b[0m: tf.math.add_2/Add:0 \u001b[34mshape\u001b[0m: (1, 26, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu_2/Relu:0 \u001b[34mshape\u001b[0m: (1, 26, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m10 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: MaxPool\u001b[35m onnx_op_name\u001b[0m: /conv_block/conv_block.11/MaxPool\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.10/Relu_output_0 \u001b[36mshape\u001b[0m: [1, 16, 26] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /conv_block/conv_block.11/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 16, 13] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: max_pool_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.input\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.relu_2/Relu:0 \u001b[34mshape\u001b[0m: (1, 26, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.filters\u001b[0m: \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.kernel_shape\u001b[0m: \u001b[34mval\u001b[0m: [2] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.strides\u001b[0m: \u001b[34mval\u001b[0m: [2] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.dilations\u001b[0m: \u001b[34mval\u001b[0m: [1] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.6.padding\u001b[0m: \u001b[34mval\u001b[0m: [0, 0] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.7.ceil_mode\u001b[0m: \u001b[34mval\u001b[0m: False \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output0\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.max_pool1d_2/MaxPool1d/Squeeze:0 \u001b[34mshape\u001b[0m: (1, 13, 16) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.2.output1\u001b[0m: \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m11 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Flatten\u001b[35m onnx_op_name\u001b[0m: /Flatten\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /conv_block/conv_block.11/MaxPool_output_0 \u001b[36mshape\u001b[0m: [1, 16, 13] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: /Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 208] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: reshape\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.tensor\u001b[0m: \u001b[34mname\u001b[0m: tf.compat.v1.transpose/transpose:0 \u001b[34mshape\u001b[0m: (1, 16, 13) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.shape\u001b[0m: \u001b[34mval\u001b[0m: [1, 208] \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.reshape/Reshape:0 \u001b[34mshape\u001b[0m: (1, 208) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m12 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Gemm\u001b[35m onnx_op_name\u001b[0m: /fc/fc.0/Gemm\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: /Flatten_output_0 \u001b[36mshape\u001b[0m: [1, 208] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.2\u001b[0m: fc.0.weight \u001b[36mshape\u001b[0m: [5, 208] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.3\u001b[0m: fc.0.bias \u001b[36mshape\u001b[0m: [5] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: output \u001b[36mshape\u001b[0m: [1, 5] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: matmul\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.x\u001b[0m: \u001b[34mname\u001b[0m: Placeholder:0 \u001b[34mshape\u001b[0m: (1, 208) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.y\u001b[0m: \u001b[34mshape\u001b[0m: (208, 5) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.3.z\u001b[0m: \u001b[34mshape\u001b[0m: (5,) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.4.alpha\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.5.beta\u001b[0m: \u001b[34mval\u001b[0m: 1.0 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (1, 5) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[32mINFO:\u001b[0m \u001b[32m13 / 13\u001b[0m\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35monnx_op_type\u001b[0m: Softmax\u001b[35m onnx_op_name\u001b[0m: sng_Softmax_0\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m input_name.1\u001b[0m: output \u001b[36mshape\u001b[0m: [1, 5] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[36m output_name.1\u001b[0m: output_softmax \u001b[36mshape\u001b[0m: [1, 5] \u001b[36mdtype\u001b[0m: float32\n",
            "\u001b[32mINFO:\u001b[0m \u001b[35mtf_op_type\u001b[0m: softmax_v2\n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.1.logits\u001b[0m: \u001b[34mname\u001b[0m: tf.__operators__.add/AddV2:0 \u001b[34mshape\u001b[0m: (1, 5) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m input.2.axis\u001b[0m: \u001b[34mval\u001b[0m: 1 \n",
            "\u001b[32mINFO:\u001b[0m \u001b[34m output.1.output\u001b[0m: \u001b[34mname\u001b[0m: tf.nn.softmax/sng_Softmax_0:0 \u001b[34mshape\u001b[0m: (1, 5) \u001b[34mdtype\u001b[0m: <dtype: 'float32'> \n",
            "\n",
            "\u001b[07mh5 output started\u001b[0m ===================================================================\n",
            "\u001b[32mjson output start...\u001b[0m\n",
            "\u001b[32mjson output finish\u001b[0m\n",
            "\u001b[32mweights.h5 output start...\u001b[0m\n",
            "\u001b[32mweights.h5 output finish\u001b[0m\n",
            "\u001b[32mweights.keras output start...\u001b[0m\n",
            "\u001b[32mweights.keras output finish\u001b[0m\n",
            "\u001b[32mweights.tf output start...\u001b[0m\n",
            "\u001b[32mweights.tf output finish\u001b[0m\n",
            "\u001b[32mkeras output start...\u001b[0m\n",
            "\u001b[32mkeras output finish\u001b[0m\n",
            "\u001b[32mh5 output start...\u001b[0m\n",
            "\u001b[32mh5 output complete!\u001b[0m\n",
            "\u001b[07msaved_model output started\u001b[0m ==========================================================\n",
            "\u001b[32msaved_model output complete!\u001b[0m\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 12, Total Ops 27, % non-converted = 44.44 %\n",
            " * 12 ARITH ops\n",
            "\n",
            "- arith.constant:   12 occurrences  (f32: 8, i32: 4)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 1)\n",
            "\u001b[32mFloat32 tflite output complete!\u001b[0m\n",
            "Summary on the non-converted ops:\n",
            "---------------------------------\n",
            " * Accepted dialects: tfl, builtin, func\n",
            " * Non-Converted Ops: 12, Total Ops 35, % non-converted = 34.29 %\n",
            " * 12 ARITH ops\n",
            "\n",
            "- arith.constant:   12 occurrences  (f16: 8, i32: 4)\n",
            "\n",
            "\n",
            "\n",
            "  (f32: 3)\n",
            "  (f32: 8)\n",
            "  (f32: 1)\n",
            "  (f32: 3)\n",
            "  (f32: 3)\n",
            "  (f32: 1)\n",
            "  (f32: 1)\n",
            "\u001b[32mFloat16 tflite output complete!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras_model = tf.keras.models.load_model('saved_model/model_float32.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "\n",
        "def representative_dataset():\n",
        "    for input_data, _ in train_dataset:\n",
        "        input_data = tf.cast(input_data, tf.float32)\n",
        "        yield [input_data[np.newaxis, ...]]\n",
        "\n",
        "# Set the optimization flag.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Enforce integer only quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "# Provide a representative dataset to ensure we quantize correctly.\n",
        "# This enables the converter to estimate a dynamic range for all the variable data.\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "with open('quantized_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_quantized_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47MRtCThPnjc",
        "outputId": "cda9f2a4-54cc-438b-f332-d81dd17bdd01"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFLite int8 model evaluation"
      ],
      "metadata": {
        "id": "dXllBGAWXboU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "input_shape = input_details[0]['shape']\n",
        "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
        "output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
        "\n",
        "def test_tflite_on_onebatch(inputs, labels):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "      test_sample = inputs[i].unsqueeze(0)\n",
        "      test_sample = test_sample / input_scale + input_zero_point\n",
        "      test_sample = test_sample.numpy().astype(input_details[0][\"dtype\"])\n",
        "\n",
        "\n",
        "      expected_output = labels[i]\n",
        "\n",
        "      interpreter.set_tensor(input_details[0]['index'], test_sample)\n",
        "      interpreter.invoke()\n",
        "\n",
        "      output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "      output_data = output_data.astype(np.float32)\n",
        "      output_data = (output_data - output_zero_point) * output_scale\n",
        "\n",
        "\n",
        "      predicted_output = np.argmax(output_data)\n",
        "\n",
        "      if predicted_output == expected_output:\n",
        "          correct_predictions += 1\n",
        "      total_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    # print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "ts_list=[]\n",
        "for inputs, labels in test_loader:\n",
        "  test_accuracy = test_tflite_on_onebatch(inputs.permute(0,2,1), labels)\n",
        "  ts_list.append(test_accuracy)\n",
        "\n",
        "print(f'Test accutacy of quantized TFLite model :{np.mean(ts_list)*100}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAnL18lGXfgV",
        "outputId": "78ae8169-def4-44fd-f580-06e78d4a0111"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accutacy of quantized TFLite model :88.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!xxd -i quantized_model.tflite > quantized_model.h"
      ],
      "metadata": {
        "id": "sJc5e9z3Y_Ij"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n1 quantized_model.h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTCZ2hqtZMvG",
        "outputId": "0f9bba98-b5a9-4f00-a777-a14ea94f9b93"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unsigned int quantized_model_tflite_len = 13912;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AVG Runtime: 33.94284 milliseconds\n",
        "\n",
        "RAM Usage: 9.4K\n",
        "\n",
        "Flash Usage: 39.1K"
      ],
      "metadata": {
        "id": "nVnNHz4qdMeO"
      }
    }
  ]
}